---
title: "Lab4"
author: "Oscar Hoffmann"
date: "2024-10-08"
output:
  pdf_document: default
  html_document: default
---

## 2.1 Implement GP Regression

Libraries

```{r}
library(kernlab)
rm(list =ls())
```

#### Task 1: Write Function for Posterior GP Simulation

Implement a function named `posteriorGP` to simulate from the posterior distribution using the squared exponential kernel.

```{r}
# Squared Exponential Kernel Function
SquaredExpKernel <- function(x1,x2,sigmaF=1,l=3){
  n1 <- length(x1)
  n2 <- length(x2)
  K <- matrix(NA,n1,n2)
  for (i in 1:n2){
    K[,i] <- sigmaF^2*exp(-0.5*( (x1-x2[i])/l)^2 )
  }
  return(K)
}

# Posterior GP Function
posteriorGP <- function(X, y, XStar, sigmaNoise, k, ...) {
  
  K <- k(X, X, ...)  # Compute the covariance matrices K(X, X)
  kStar <- k(X, XStar, ...) # Compute covariance between training and test inputs
  
  # Step 2 in algo
  #--------------------
  K_y <- K + sigmaNoise^2 * diag(length(X)) # Add noise variance to diagonal, K_y= covariance matrix of y
  L <- t(chol(K_y))   # Compute Cholesky decomposition, to get lower triangular L we take t()
  alpha <- solve(t(L), solve(L, y))   # Solve for alpha
  #---------------------
  
  # Step 4 in algo
  #--------------------
  fStar_mean <- t(kStar) %*% alpha   # Compute posterior mean
  v <- solve(L, kStar)   # Compute v = solve(L, kStar)
  #-------------------
  
  # Step 6 in algo
  #-------------------
  V_fStar <- k(XStar, XStar, ...) - t(v) %*% v # pred variance (cov matrix)
  #-------------------
  
  # Return posterior mean and variance
  return(list(mean = fStar_mean, variance = V_fStar))
}

```

#### Task 2: Update Posterior with Single Observation

Let prior hyperparameters be σf = 1 and l = 0.3, update with (x, y) = (0.4, 0.719), and plot the posterior mean with 95% bands.

```{r}
# Plotting Function
plotGP <- function(XStar, res, X_train, y_train, title) {
  
  # Extract posterior mean and variance
  pos_mean <- res$mean
  pos_var <- diag(res$variance)

  # Compute 95% confidence intervals
  lower_bound <- pos_mean - 1.96 * sqrt(pos_var)
  upper_bound <- pos_mean + 1.96 * sqrt(pos_var)
  
  # Plot the posterior mean and 95% probability bands
  plot(XStar, pos_mean, type = "l", lwd = 2,
    ylim = range(c(lower_bound, upper_bound, y_train)),
    ylab = "f(x)", xlab = "x", main = title)
  
  # Add the confidence intervals
  lines(XStar, lower_bound, lty = 2)
  lines(XStar, upper_bound, lty = 2)
  # Plot the training data points
  points(X_train, y_train, pch = 19, col = "red")
}
```

Put plotting into a function for easy reuse

```{r}
# Single observation
X <- c(0.4)
y <- c(0.719)

# Test inputs over the interval [-1, 1]
XStar <- seq(-1, 1, length.out = 100)

# Hyperparameters
sigmaF <- 1        # σ_f
ell <- 0.3         # length-scale l
sigmaNoise <- 0.1  # σ_n

# Call posteriorGP
res <- posteriorGP(X, y, XStar, sigmaNoise, k = SquaredExpKernel, sigmaF = sigmaF, l = ell)

plotGP(XStar, res, X, y, "Posterior Mean and 95% Probability Bands")
```

#### Task 3: Update with Another Observation

Update your posterior from Task 2 with (x, y) = (-0.6, -0.044), plot the posterior mean, and include 95% probability bands.

```{r}
# Updated training data
X <- c(0.4, -0.6)
y <- c(0.719, -0.044)

# Same XStar and hyperparameters as in task2
res <- posteriorGP(X, y, XStar, sigmaNoise, k = SquaredExpKernel, sigmaF = sigmaF, l = ell)

plotGP(XStar, res, X, y, "Posterior Mean and 95% Probability Bands")
```

#### Task 4: Compute Posterior with All Observations

Compute the posterior distribution of f using all five data points and plot the posterior mean with 95% bands.

```{r}
# Step 4: Posterior with all five observations
X <- c(-1.0, -0.6, -0.2, 0.4, 0.8)
y <- c(0.768, -0.044, -0.940, 0.719, -0.664)

res <- posteriorGP(X, y, XStar, sigmaNoise, k = SquaredExpKernel, sigmaF = sigmaF, l = ell)
plotGP(XStar, res, X, y, "Posterior Mean and 95% Probability Bands, 5 obs")
```

#### Task 5: Compare Hyperparameter Settings

Repeat Task 4 using different hyperparameters: σf = 1 and l = 1, compare the results.

```{r}
# Hyperparameters
sigmaF <- 1     # σ_f
ell <- 1        # length-scale l

res <- posteriorGP(X, y, XStar, sigmaNoise, k = SquaredExpKernel, sigmaF = sigmaF, l = ell)
plotGP(XStar, res, X, y, "Posterior Mean and 95% Probability Bands, new hyp params")
```

When increasing ell from 0.3 to 1 we get more smoothness which is expected.

## 2.2 GP Regression with kernlab

#### Task 1: Define Kernel and Compute Covariance Matrix

Define your own squared exponential kernel and use the `kernelMatrix` function to compute the covariance matrix for given vectors.

```{r}
data = read.csv("https://github.com/STIMALiU/AdvMLCourse/raw/master/GaussianProcess/Code/TempTullinge.csv", header=TRUE, sep=";")

time = seq(1,2190, 5)
day = seq(1,365, 5)

data_sampled = data[time,]
data_sampled$time = time
data_sampled$day = day
```

#### Task 2: Estimate GP Model with gausspr

Use the `gausspr` function with σf = 20 and l = 100, estimate the Gaussian process regression model, and plot the posterior mean.

```{r}
# Placeholder for Gaussian process regression with gausspr
```

#### Task 3: Implement Algorithm 2.1 for GP Regression

Implement Algorithm 2.1 from Rasmussen and Williams' book to compute the posterior mean and variance.

```{r}
# Placeholder for implementation of Algorithm 2.1
```

#### Task 4: Estimate Model with day Variable

Estimate the model using `gausspr` with the day variable, superimpose the posterior mean on the previous model.

```{r}
# Placeholder for estimating model with day variable
```

#### Task 5: Implement Locally Periodic Kernel

Implement the extended squared exponential kernel with a periodic kernel and compare the results.

```{r}
# Placeholder for implementing locally periodic kernel
```

## 2.3 GP Classification with kernlab

#### Task 1: Fit GP Classification Model

Use the `kernlab` package to fit a Gaussian process classification model for fraud, and plot contours of prediction probabilities.

```{r}
# Placeholder for Gaussian process classification model
```

#### Task 2: Make Predictions for Test Set

Make predictions for the test set and compute the accuracy.

```{r}
# Placeholder for making predictions on test set
```

#### Task 3: Train Model with All Covariates

Train a model using all four covariates and compare the accuracy to the model with only two covariates.

\`\`\`{r} \# Placeholder for training model with all covariates and comparing accuracy
