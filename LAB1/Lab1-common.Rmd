---
title: "TDDE15-Lab 1"
author: "frera064, oscho091, hjaoh082, olosw720"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

**Contribution**

All four members solved the problems on their own and discussed the answers. Task 1: frera064, Task 2 & 3: hjaoh082, Task 4: oscho091, Task 5: olosw720

**Libraries and data imports.**

```{r}
library(bnlearn)
library(gRain)
data("asia")
```

**Task 1**

*Show that multiple runs of the hill-climbing algorithm can return non-equivalent Bayesian network (BN) structures. Explain why this happens. Use the Asia dataset which is in- cluded in the **bnlearn** package. To load the data, run **data("asia")**. Recall from the lectures that the concept of non-equivalent BN structures has a precise meaning.*

***Hint:** Check the function **hc** in the **bnlearn** package. Note that you can specify the [initial structure, the number of random restarts, the score, and the equivalent sam- ple size (a.k.a imaginary sample size) in the BDeu score]{.underline}. You may want to use these options to answer the question. You may also want to use the functions **plot**, **arcs**, **vstructs**, **cpdag** and **all.equal***

```{r}
random_restarts <- c(0,10,100)
score <- c("bic","aic","bde")
im_sample_size <- c(1,10,100)
init <- random.graph(colnames(asia))#model2network("[S][A|S][T|S][L|S][B|S][E|S][X|S][D|S]")
```

*Initialized graph or not*

```{r fig.height=5, fig.width=8}
init_dag <- hc(asia, start = init)
uninit_dag <- hc(asia)

par(mfrow = c(1,2))
plot(init_dag, main = "Random initialization for S")
plot(uninit_dag, main = "No initialization")
```

*Different random restarts*

```{r fig.width = 8, fig.height = 5}
restart_dag1 <- hc(asia, restart = random_restarts[1])
restart_dag2 <- hc(asia, restart = random_restarts[2])
restart_dag3 <- hc(asia, restart = random_restarts[3])

par(mfrow = c(1,3))
plot(restart_dag1, main = random_restarts[1])
plot(restart_dag2, main = random_restarts[2])
plot(restart_dag3, main = random_restarts[3])
#all.equal(cpdag(restart_dag1), cpdag(restart_dag2)) #Compares equivalence
#all.equal(restart_dag1, restart_dag2)
```

*Different scores*

```{r fig.width = 8, fig.height = 5}
score_dag1 <- hc(asia, score = score[1])
score_dag2 <- hc(asia, score = score[2])
score_dag3 <- hc(asia, score = score[3])

par(mfrow = c(1,3))
plot(score_dag1, main = score[1])
plot(score_dag2, main = score[2])
plot(score_dag3, main = score[3])
```

*Different imaginary sample sizes (iss)*

```{r fig.width = 8, fig.height = 5}
im_sample_dag1 <- hc(asia, score = "bde", iss = im_sample_size[1])
im_sample_dag2 <- hc(asia, score = "bde", iss = im_sample_size[2])
im_sample_dag3 <- hc(asia, score = "bde", iss = im_sample_size[3])

par(mfrow = c(1,3))
plot(im_sample_dag1, main = im_sample_size[1])
plot(im_sample_dag2, main = im_sample_size[2])
plot(im_sample_dag3, main = im_sample_size[3])
#all.equal(cpdag(im_sample_dag1), cpdag(im_sample_dag3)) #Compares equivalence
```

*Discussion*

The hill climbing algorithm is a heuristic and can find local optimas that are not global optimum and therefore produce different non-equivalent bns. We see very little difference between different random restart values, this is probably because the nr of nodes are so small that few local optima exist other than global. The regularization term iss on the other hand has a large impact on the graph drastically reducing the number of arcs for lower iss values.

## 2

```{r}
rm(list=ls())
library(bnlearn)
library(gRain)
set.seed(1)
data("asia")

n=dim(asia)[1]
id=sample(1:n, floor(n*0.8))
train_data=asia[id,]
test_data=asia[-id,]

bn = hc(train_data)
dag = model2network("[A][S][T|A][L|S][B|S][D|B:E][E|T:L][X|E]")
plot(dag)
fit = bn.fit(x = bn, data = train_data)
true_fit = bn.fit(x = dag, data = train_data)

grain = as.grain(fit)
true_grain = as.grain(true_fit)

compile = compile(grain)
true_compile = compile(true_grain)

predict = function (network){
  predictions = c()
  for (i in 1:nrow(test_data)) {
    evidence = setEvidence(network, colnames(test_data)[-2], as.vector(as.matrix(test_data[i, -2])))
    query = querygrain(evidence, colnames(test_data)[2])$S
    
    predictions = c(predictions, ifelse(query["yes"]>query["no"], "yes", "no"))
  }
  return(predictions)
}

table(predict(compile), test_data$S)
table(predict(true_compile), test_data$S)
graphviz.plot(dag)
graphviz.plot(bn)
```

The confusion matrices are very similar. Only missing from true is A-\>T in the dags.

## 3

"Find the minimal set of nodes that separates a given node from the rest. This set is called the Markov blanket of the given node."

```{r}
mb = mb(fit, c("S"))
true_mb = mb(true_fit, c("S"))

predict_mb = function (network, m_b){
  predictions = c()
  for (i in 1:nrow(test_data)) {
    evidence = setEvidence(network, m_b, as.vector(as.matrix(test_data[i, m_b])))
    query = querygrain(evidence, colnames(test_data)[2])$S
    
    predictions = c(predictions, ifelse(query["yes"]>query["no"], "yes", "no"))
  }
  return(predictions)
}

table(predict_mb(compile, mb), test_data$S)
table(predict_mb(true_compile, true_mb), test_data$S)
```

The confusion matrices are once again similar. Even identical now.

**4**

```{r}
#Q4, 
# Naive Bayes classifier
nb <- model2network("[S][A|S][B|S][D|S][E|S][L|S][T|S][X|S]")
nbc <- bn.fit(x = nb, data = train_data) 

# Transform into gRain obj
nbc <- as.grain(nbc)
nbc <- compile(nbc)

# Confusion matrix
table(predict(nbc), test_data$S)
plot(nb)
```

```         
       no yes
  no  359 180
  yes 154 307
```

**Task 5:** Explain why you obtain the same or different results in the exercises (2-4).

The Markov blanket approach and the full Bayesian network yielded similar results because both rely on a more fitted structure of dependencies in the network, with the Markov blanket focusing on the minimal sufficient set of variables needed to predict S. On the other hand, the naive Bayes classifier produces different results because it makes the unrealistic assumption that all predictive variables are conditionally independent given S, leading to a loss of dependency information.
