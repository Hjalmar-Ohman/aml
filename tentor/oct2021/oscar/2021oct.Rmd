---
title: "TDDE15 - Exam"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output:
  pdf_document: default
  latex_engine: lualatex
---

## Graphical Models (5 p)

```{r}
rm(list = ls())
library(bnlearn)
library(gRain)
data("asia")

```

```{r}
true_dag = model2network("[A][S][T|A][L|S][B|S][D|B:E][E|T:L][X|E]")
net = bn.fit(true_dag, data = asia)
true_dag_grain = as.grain(net)
true_dag_compiled = compile(true_dag_grain)
graphviz.plot(true_dag)
```

```{r}
samples = matrix(nrow = 1000, ncol = 8)
colnames(samples) = c("A", "S", "T", "L","B", "E","X", "D")

for (i in 1:1000) {
   samples[i, "A"] <- sample(c("no", "yes"), size = 1, prob = net$A$prob)
   samples[i, "S"] <- sample(c("no", "yes"), size = 1, prob = net$S$prob)
   samples[i, "T"] <- sample(c("no", "yes"), size = 1, prob = net$L$prob[, samples[i, "A"]])
   samples[i, "L"] <- sample(c("no", "yes"), size = 1, prob = net$prob[, samples[i, "S"]])
   samples[i, "B"] <- sample(c("no", "yes"), size = 1, prob = net$prob[, samples[i, "S"]])
   samples[i, "E"] <- sample(c("no", "yes"), size = 1, prob = net$probs[,samples[i, "L"], samples[i, "T"]])
   samples[i, "D"] <- sample(c("no", "yes"), size = 1, prob = net$D$prob[, samples[i, "B"],samples[i, "E"]])
}

foo<-samples[which(samples[,8]=="yes"),2]
table(foo)/length(foo)

net<-as.grain(net)
net<-compile(net)
net<-setEvidence(net,nodes=c("D"),states=c("yes"))
querygrain(net,c("S"))
```

## Hidden Markov Models

-   p(healthy D2 ) = 0.9

-   p( infected D2 ) = 0.8

-   if infected remains infected for two days

-   p( test positive ) = 0.6

-   p( test negative ) = 0.7

hhi = 0.1

hhh = 0.9

ii = 0.8

|     | H   | S1  | S2  |
|-----|-----|-----|-----|
| H   | 0.9 | 0.1 | 0.0 |
| S1  | 0   | 0.8 | 0.2 |
| S2  | 0.2 | 0.0 | 0.8 |

```{r}


# Load the HMM package
library(HMM)
library(entropy)

# Define the hidden states and observation symbols
states <- c("H", "S1", "S2")
symbols <- c("H", "S")

start_probs <- c(.5, .5, 0)

trans_probs = matrix(c(.9,.1,0,
                       0,.8,.2,
                       .2,0,.8
                       ), nrow=length(states), ncol=length(states), byrow = TRUE)

colnames(trans_probs) = states
colnames(trans_probs) = states

emission_probs = matrix(c(.7,.3,
                          .4,.6,
                          .4,.6
), nrow=length(states), ncol=length(symbols), byrow = TRUE)

colnames(emission_probs) = symbols
rownames(emission_probs) = states

# Initialize the Hidden Markov Model
hmm_model <- initHMM(
  States = states,           # vector of states
  Symbols = symbols,         # vector of observation symbols
  startProbs = start_probs,  # Initial state probabilities
  transProbs = trans_probs,  # Transition probabilities matrix
  emissionProbs = emission_probs  # Emission probabilities matrix
)





set.seed(12345)
simHMM(hmm_model, length = 100)
```

## Reinforcement learning

```{r}

# given from labs
set.seed(1234)
library(ggplot2)

arrows <- c("^", ">", "v", "<")
action_deltas <- list(c(1,0), # up
                      c(0,1), # right
                      c(-1,0), # down
                      c(0,-1)) # left

vis_environment <- function(iterations=0, epsilon = 0.5, alpha = 0.1, gamma = 0.95, beta = 0){

  df <- expand.grid(x=1:H,y=1:W)
  foo <- mapply(function(x,y) ifelse(reward_map[x,y] == 0,q_table[x,y,1],NA),df$x,df$y)
  df$val1 <- as.vector(round(foo, 2))
  foo <- mapply(function(x,y) ifelse(reward_map[x,y] == 0,q_table[x,y,2],NA),df$x,df$y)
  df$val2 <- as.vector(round(foo, 2))
  foo <- mapply(function(x,y) ifelse(reward_map[x,y] == 0,q_table[x,y,3],NA),df$x,df$y)
  df$val3 <- as.vector(round(foo, 2))
  foo <- mapply(function(x,y) ifelse(reward_map[x,y] == 0,q_table[x,y,4],NA),df$x,df$y)
  df$val4 <- as.vector(round(foo, 2))
  foo <- mapply(function(x,y) 
    ifelse(reward_map[x,y] == 0,arrows[GreedyPolicy(x,y)],reward_map[x,y]),df$x,df$y)
  df$val5 <- as.vector(foo)
  foo <- mapply(function(x,y) ifelse(reward_map[x,y] == 0,max(q_table[x,y,]),
                                     ifelse(reward_map[x,y]<0,NA,reward_map[x,y])),df$x,df$y)
  df$val6 <- as.vector(foo)
  
  print(ggplot(df,aes(x = y,y = x)) +
          scale_fill_gradient(low = "white", high = "green", na.value = "red", name = "") +
          geom_tile(aes(fill=val6)) +
          geom_text(aes(label = val1),size = 4,nudge_y = .35,na.rm = TRUE) +
          geom_text(aes(label = val2),size = 4,nudge_x = .35,na.rm = TRUE) +
          geom_text(aes(label = val3),size = 4,nudge_y = -.35,na.rm = TRUE) +
          geom_text(aes(label = val4),size = 4,nudge_x = -.35,na.rm = TRUE) +
          geom_text(aes(label = val5),size = 10) +
          geom_tile(fill = 'transparent', colour = 'black') + 
          ggtitle(paste("Q-table after ",iterations," iterations\n",
                        "(epsilon = ",epsilon,", alpha = ",alpha,"gamma = ",gamma,", beta = ",beta,")")) +
          theme(plot.title = element_text(hjust = 0.5)) +
          scale_x_continuous(breaks = c(1:W),labels = c(1:W)) +
          scale_y_continuous(breaks = c(1:H),labels = c(1:H)))
}

GreedyPolicy <- function(x, y){
  # Get the Q-values for all actions at state (x, y)
  q_values <- q_table[x, y, ]
  
  # Find the max Q-value
  max_q <- max(q_values)
  
  # Identify all actions with maximum Q-value
  max_actions <- which(q_values == max_q)
  
  # Check and resolve ties
  if (length(max_actions) > 1) {
    action <- sample(max_actions, 1)
  } else {
    action <- max_actions
  }
  return(action)
}

EpsilonGreedyPolicy <- function(x, y, epsilon){
  # Generate a random numb
  rand_num <- runif(1)
  if (rand_num < epsilon){
   #select a random action
    action <- sample(1:4, 1)
  } else {
  # use the greedy policy
    action <- GreedyPolicy(x, y)
  }
  return(action)
}

transition_model <- function(x, y, action, beta){
  delta <- sample(-1:1, size = 1, prob = c(0.5*beta,1-beta,0.5*beta))
  final_action <- ((action + delta + 3) %% 4) + 1
  foo <- c(x,y) + unlist(action_deltas[final_action])
  foo <- pmax(c(1,1),pmin(foo,c(H,W)))
  return (foo)
}

q_learning <- function(start_state, epsilon = 0.5, alpha = 0.1, gamma = 0.95, beta = 0, train){
  
  # Perform one episode of Q-learning. The agent should move around in the 
  # environment using the given transition model and update the Q-table.
  # The episode ends when the agent reaches a terminal state.
  # 
  # Args:
  #   start_state: array with two entries, describing the starting position of the agent.
  #   epsilon (optional): probability of acting randomly.
  #   alpha (optional): learning rate.
  #   gamma (optional): discount factor.
  #   beta (optional): slipping factor.
  #   reward_map (global variable): a HxW array containing the reward given at each state.
  #   q_table (global variable): a HxWx4 array containing Q-values for each state-action pair.
  # 
  # Returns:
  #   reward: reward received in the episode.
  #   correction: sum of the temporal difference correction terms over the episode.
  #   q_table (global variable): Recall that R passes arguments by value. So, q_table being
  #   a global variable can be modified with the superassigment operator <<-.
  
  # Initialize state
  x <- start_state[1]
  y <- start_state[2]
  
  # Initialize variables to track episode reward and TD corrections
  episode_reward <- 0
  episode_correction <- 0
  
  repeat{
    # Choose an action A with epsilon-greedy policy
    action <- EpsilonGreedyPolicy(x, y, epsilon)
    
    # Observe next state S' & reward R after taking action A
    next_state <- transition_model(x, y, action, beta)
    x_new <- next_state[1]
    y_new <- next_state[2]
    R <- reward_map[x_new, y_new]
    
    # Get current Q-value Q(S, A)
    Q_SA <- q_table[x, y, action]
    
    # if R != 0 it means next state is terminal, this check if for when the terminal state
    # is the first action and the end of the loop hasn't been reached
    if (R != 0){
      max_QSAprime <- 0
    } else {
      # Compute max Q(S', a') over all possible actions a'
      max_QSAprime <- max(q_table[x_new, y_new, ])
    }
    
    # Calc TD correction term
    TD_correction <- R + gamma * max_QSAprime - Q_SA
    episode_correction <- episode_correction + TD_correction*train
    
    # Update Q(S, A)
    q_table[x, y, action] <<- Q_SA + alpha * TD_correction*train
    
    # Ep reward accumulated
    episode_reward <- episode_reward + R
    
    # Next state
    x <- x_new
    y <- y_new
    
    # Check if the episode has ended (terminal state)
    if (R != 0){
      return (c(episode_reward, episode_correction))
    }
  }
}
```

```{r}
# environment B
H <- 7
W <- 8

reward_map <- matrix(0, nrow = H, ncol = W)
reward_map[1,] <- -1
reward_map[7,] <- -1
reward_map[4,5] <- 5
reward_map[4,8] <- 10

q_table <- array(0,dim = c(H,W,4))

vis_environment()

MovingAverage <- function(x, n){
  
  cx <- c(0,cumsum(x))
  rsum <- (cx[(n+1):length(cx)] - cx[1:(length(cx) - n)]) / n
  
  return (rsum)
}

mreward = NULL
#----------epsilon = 0.5 ---------------------#  
for(i in c(0.1, 0.25, 0.5)){
  for(j in c(0.5,0.75,0.95)){
  q_table <- array(0,dim = c(H,W,4))
  reward <- NULL
  correction <- NULL
   for(k in 1:31000){
    if(k <=30000){
    foo <- q_learning(epsilon = i, gamma = j, start_state = c(4,1), train = 1)
    } else {
    foo <- q_learning(epsilon = i, gamma = j, start_state = c(4,1), train =0)
    }
    reward <- c(reward,foo[1])
  }
  
  vis_environment(k, gamma = j, epsilon = i)
  mreward <- c(mreward,mean(reward))
}
}

```
