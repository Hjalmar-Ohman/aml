### 1

```{r}
rm(list=ls())
library(bnlearn)
library(gRain)
set.seed(12345)
data("asia")

true_dag = model2network("[A][S][T|A][L|S][B|S][D|B:E][E|T:L][X|E]")
graphviz.plot(true_dag)

net = bn.fit(x = true_dag, data = asia)

samples = data.frame(A = rep(0, 1000), S  = rep(0, 1000), T = rep(0, 1000), L = rep(0, 1000), E = rep(0, 1000), B = rep(0, 1000), X = rep(0, 1000), D = rep(0, 1000))


for (i in 1:nrow(samples)) {
  a = sample(1:2, 1, prob=net$A$prob)
  s = sample(1:2, 1, prob=net$S$prob)
  t = sample(1:2, 1, prob=net$T$prob[, a])
  l = sample(1:2, 1, prob=net$L$prob[, s])
  b = sample(1:2, 1, prob=net$B$prob[, s])
  e = sample(1:2, 1, prob=net$E$prob[, l, t])
  d = sample(1:2, 1, prob=net$D$prob[, b, e])
  x = sample(1:2, 1, prob=net$X$prob[, e])
  
  samples[i,] = c(a,s,t,l,b,e,d,x)
}

# p(S|D = 1)
mean(samples$S[samples$D == 2]-1)

net = compile(as.grain(net))
evidence = setEvidence(net, nodes = "D", states = "yes")
query = querygrain(evidence, "S")
query$S
```

### 2
```{r}
rm(list=ls())
set.seed(1)
library(HMM)
states = 1:3
symbols = 1:2
start_probs = c(0.5, 0, 0.5)
# Define the transition probabilities
trans_probs = matrix(c(
  #1a #1b #2
  .8, .2,  0,
  0, .8, .2,
  .1, 0, .9
), byrow = TRUE, ncol = length(states), nrow = length(states))

# Define the emission probabilities
emission_probs = matrix(c(
  #1 #2
  .7, .3,      #1a
  .7, .3,      #1b
  .4, .6      #2
), byrow = TRUE, nrow=length(states), ncol=length(symbols))


hmm_model = initHMM(States=states, Symbols=symbols, startProbs=start_probs,
transProbs=trans_probs, emissionProbs=emission_probs)
print(hmm_model)
simHMM(hmm_model, 100)
```

### 3
```{r}
rm(list = ls())
# By Jose M. Peña and Joel Oskarsson.
# For teaching purposes.
# jose.m.pena@liu.se.
#####################################################################################################
# Q-learning
#####################################################################################################
# install.packages("ggplot2")
# install.packages("vctrs")
library(ggplot2)
set.seed(1234)
arrows <- c("ˆ", ">", "v", "<")
action_deltas <- list(c(1, 0), # up
                      c(0, 1), # right
                      c(-1, 0), # down
                      c(0, -1)) # left

vis_environment <- function(iterations = 0,
                            epsilon = 0.5,
                            alpha = 0.1,
                            gamma = 0.95,
                            beta = 0) {
  # Visualize an environment with rewards.
  # Q-values for all actions are displayed on the edges of each tile.
  # The (greedy) policy for each state is also displayed.
  #
  # Args:
  # iterations, epsilon, alpha, gamma, beta (optional): for the figure title.
  # reward_map (global variable): a HxW array containing the reward given at each state.
  # q_table (global variable): a HxWx4 array containing Q-values for each state-action pair.
  # H, W (global variables): environment dimensions.
  df <- expand.grid(x = 1:H, y = 1:W)
  foo <- mapply(function(x, y)
    ifelse(reward_map[x, y] == 0, q_table[x, y, 1], NA), df$x, df$y)
  df$val1 <- as.vector(round(foo, 2))
  foo <- mapply(function(x, y)
    ifelse(reward_map[x, y] == 0, q_table[x, y, 2], NA), df$x, df$y)
  df$val2 <- as.vector(round(foo, 2))
  foo <- mapply(function(x, y)
    ifelse(reward_map[x, y] == 0, q_table[x, y, 3], NA), df$x, df$y)
  df$val3 <- as.vector(round(foo, 2))
  foo <- mapply(function(x, y)
    ifelse(reward_map[x, y] == 0, q_table[x, y, 4], NA), df$x, df$y)
  df$val4 <- as.vector(round(foo, 2))
  foo <- mapply(function(x, y)
    ifelse(reward_map[x, y] == 0, arrows[GreedyPolicy(x, y)], reward_map[x, y]),
    df$x,
    df$y)
  df$val5 <- as.vector(foo)
  foo <- mapply(function(x, y)
    ifelse(
      reward_map[x, y] == 0,
      max(q_table[x, y, ]),
      ifelse(reward_map[x, y] < 0, NA, reward_map[x, y])
    ), df$x, df$y)
  df$val6 <- as.vector(foo)
  print(
    ggplot(df, aes(x = y, y = x)) +
      scale_fill_gradient(
        low = "white",
        high = "green",
        na.value = "red",
        name = ""
      ) +
      geom_tile(aes(fill = val6)) +
      geom_text(
        aes(label = val1),
        size = 4,
        nudge_y = .35,
        na.rm = TRUE
      ) +
      geom_text(
        aes(label = val2),
        size = 4,
        nudge_x = .35,
        na.rm = TRUE
      ) +
      geom_text(
        aes(label = val3),
        size = 4,
        nudge_y = -.35,
        na.rm = TRUE
      ) +
      geom_text(
        aes(label = val4),
        size = 4,
        nudge_x = -.35,
        na.rm = TRUE
      ) +
      geom_text(aes(label = val5), size = 10) +
      geom_tile(fill = 'transparent', colour = 'black') +
      ggtitle(
        paste(
          "Q-table after ",
          iterations,
          " iterations\n",
          "(epsilon = ",
          epsilon,
          ", alpha = ",
          alpha,
          "gamma = ",
          gamma,
          ", beta = ",
          beta,
          ")"
        )
      ) +
      theme(plot.title = element_text(hjust = 0.5)) +
      scale_x_continuous(breaks = c(1:W), labels = c(1:W)) +
      scale_y_continuous(breaks = c(1:H), labels = c(1:H))
  )
}

GreedyPolicy <- function(x, y) {
  # Get a greedy action for state (x,y) from q_table.
  #
  # Args:
  # x, y: state coordinates.
  # q_table (global variable): a HxWx4 array containing Q-values for each state-action pair.
  #
  # Returns:
  # An action, i.e. integer in {1,2,3,4}.
  # Your code here.
  q_values = q_table[x, y, ]
  best_actions = which(q_values == max(q_values))
  if (length(best_actions) > 1) {
    action = sample(best_actions, 1)
  } else {
    action = best_actions
  }
  return (action)
}

EpsilonGreedyPolicy <- function(x, y, epsilon) {
  # Get an epsilon-greedy action for state (x,y) from q_table.
  #
  # Args:
  # x, y: state coordinates.
  # epsilon: probability of acting randomly.
  #
  # Returns:
  # An action, i.e. integer in {1,2,3,4}.
  # Your code here.
  if (epsilon >= runif(1)) {
    action = sample(1:4, 1)
  } else {
    action = GreedyPolicy(x, y)
  }
  return (action)
}
transition_model <- function(x, y, action, beta) {
  # Computes the new state after given action is taken. The agent will follow the action
  # with probability (1-beta) and slip to the right or left with probability beta/2 each.
  #
  # Args:
  # x, y: state coordinates.
  # action: which action the agent takes (in {1,2,3,4}).
  # beta: probability of the agent slipping to the side when trying to move.
  # H, W (global variables): environment dimensions.
  #
  # Returns:
  # The new state after the action has been taken.
  delta <- sample(-1:1,
                  size = 1,
                  prob = c(0.5 * beta, 1 - beta, 0.5 * beta))
  final_action <- ((action + delta + 3) %% 4) + 1
  foo <- c(x, y) + unlist(action_deltas[final_action])
  foo <- pmax(c(1, 1), pmin(foo, c(H, W)))
  return (foo)
}

q_learning <- function(start_state,
                       epsilon = 0.5,
                       alpha = 0.1,
                       gamma = 0.95,
                       beta = 0) {
  #Initialize
  current_state = start_state
  episode_correction = 0
  reward = 0
  repeat {
    # Current state
    x = current_state[1]
    y = current_state[2]
    # Action
    action = EpsilonGreedyPolicy(x, y, epsilon)
    # Update state
    next_state = transition_model(x, y, action, beta)
    next_x = next_state[1]
    next_y = next_state[2]
    # Reward
    reward = reward_map[next_x, next_y]
    # New max Q value
    max_q_next = max(q_table[next_x, next_y, ])
    # Correction
    correction = reward + gamma * max_q_next - q_table[x, y, action]
    # Update q_table based on correction
    q_table[x, y, action] <<- q_table[x, y, action] + alpha * correction
    # Accumulate corrections
    episode_correction = episode_correction + correction
    # Update state
    current_state = next_state
    # End the episode if a terminal state (non-zero reward) is reached
    if (reward != 0) {
      break
    }
  }
  return (c(reward, episode_correction))
}

set.seed(1234)
H <- 7
W <- 8
reward_map <- matrix(0, nrow = H, ncol = W)
reward_map[1, ] <- -1
reward_map[7, ] <- -1
reward_map[4, 5] <- 5
reward_map[4, 8] <- 10
q_table <- array(0, dim = c(H, W, 4))
```

```{r}
q_running <- function(start_state,
                       epsilon = 0.5,
                       alpha = 0.1,
                       gamma = 0.95,
                       beta = 0) {
  #Initialize
  current_state = start_state
  #episode_correction = 0
  reward = 0
  repeat {
    # Current state
    x = current_state[1]
    y = current_state[2]
    # Action
    action = EpsilonGreedyPolicy(x, y, epsilon)
    # Update state
    next_state = transition_model(x, y, action, beta)
    next_x = next_state[1]
    next_y = next_state[2]
    # Reward
    reward = reward_map[next_x, next_y]
    # New max Q value
    #max_q_next = max(q_table[next_x, next_y, ])
    # Correction
    #correction = reward + gamma * max_q_next - q_table[x, y, action]
    # Update q_table based on correction
    #q_table[x, y, action] <<- q_table[x, y, action] + alpha * correction
    # Accumulate corrections
    #episode_correction = episode_correction + correction
    # Update state
    current_state = next_state
    # End the episode if a terminal state (non-zero reward) is reached
    if (reward != 0) {
      break
    }
  }
  return (c(reward))#, episode_correction))
}

for(g in c(0.5, 0.75, 0.95)) {
  for(e in c(0.1, 0.25, 0.5)) {
    q_table <- array(0, dim = c(H, W, 4))
    for (i in 1:30000)
      foo <- q_learning(gamma = g,
                        epsilon = e,
                        start_state = c(4, 1))
    rewards = NULL
    
    for (i in 1:1000){
      foo <- q_running(gamma = g,
                        epsilon = e,
                        start_state = c(4, 1))
      rewards = c(rewards, foo)
    }
    
    vis_environment(mean(rewards), gamma = g, epsilon = e)
  }
}

```