---
output:
  word_document: default
  html_document: default
  pdf_document: default
---

## 1. Graphical models

1)  

```{r}

library(bnlearn)
library(gRain)
dag = model2network("[C][D|C][A|C][Y|A:C]")

graphviz.plot(dag)
```

2)  

```{r}

countC_ND = 0
countD_NC = 0
for (i in 1:1000) {
  dag = model2network("[C][D|C][A|C][Y|A:C]")
  # C
  s = runif(1)
  cptC = c(s, 1-s)
  dim(cptC) = c(2)
  dimnames(cptC) = list(C = c("C1", "C0"))

  # D | C
  s1 = runif(1)
  s2 = runif(1)
  cptD = matrix(c(s1, 1-s1,  
                 s2, 1-s2),
               nrow = 2)
  dimnames(cptD) = list(D = c("D1", "D0"), C = c("C1", "C0"))

   # A | C
  s1 = runif(1)
  s2 = runif(1)
  cptA = matrix(c(s1, 1-s1,  
                 s2, 1-s2),
               nrow = 2)
  dimnames(cptA) = list(A = c("A1", "A0"), C = c("C1", "C0"))
  
     # Y | A,C
  s1 = runif(1)
  s2 = runif(1)
  s3 = runif(1)
  s4 = runif(1)
  cptY = matrix(c(s1,1-s1,
                  s2,1-s2,
                  s3,1-s3,
                  s4,1-s4),
                )
  dim(cptY) = c(2,2,2)
  dimnames(cptY) = list(Y = c("Y1", "Y0"), A = c("A1", "A0"), C = c("C1", "C0"))

  fit = custom.fit(dag, list(C = cptC,D = cptD,A = cptA, Y = cptY))
  model = compile(as.grain(fit))
  
  # for p(y|a,c)
  Y1_A1C1 <- querygrain(setEvidence(object = model, nodes = c("A", "C"), states = c("A1", "C1")),nodes = "Y")$Y[1]
  Y1_A1C0 <- querygrain(setEvidence(object = model, nodes = c("A", "C"), states = c("A1", "C0")),nodes = "Y")$Y[1]
  Y1_A0C1 <- querygrain(setEvidence(object = model, nodes = c("A", "C"), states = c("A0", "C1")),nodes = "Y")$Y[1]
  Y1_A0C0 <- querygrain(setEvidence(object = model, nodes = c("A", "C"), states = c("A0", "C0")),nodes = "Y")$Y[1]
  
  # for p(y|a,d)
  Y1_A1D1 <- querygrain(setEvidence(object = model, nodes = c("A", "D"), states = c("A1", "D1")),nodes = "Y")$Y[1]
  Y1_A1D0 <- querygrain(setEvidence(object = model, nodes = c("A", "D"), states = c("A1", "D0")),nodes = "Y")$Y[1]
  Y1_A0D1 <- querygrain(setEvidence(object = model, nodes = c("A", "D"), states = c("A0", "D1")),nodes = "Y")$Y[1]
  Y1_A0D0 <- querygrain(setEvidence(object = model, nodes = c("A", "D"), states = c("A0", "D0")),nodes = "Y")$Y[1]
 
  if(Y1_A1C1 >= Y1_A1C0 && Y1_A0C1 >= Y1_A0C0){
    if(Y1_A1C1 <= Y1_A1C0 && Y1_A0C1 <= Y1_A0C0){
      monotoneC = TRUE
    }
    } else {
    monotoneC = FALSE
  }
 
      monotoneD = FALSE
   if(Y1_A1D1 >= Y1_A1D0 && Y1_A0D1 >= Y1_A0D0){
    if(Y1_A1D1 <= Y1_A1D0 && Y1_A0D1 <= Y1_A0D0){
      monotoneD = TRUE
    }
    } else {
    monotoneD = FALSE
  }
 
 
  if(monotoneC && !monotoneD){
    countC_ND = countC_ND +1
  }else if(!monotoneC && monotoneD){
    countD_NC = countD_NC +1 
  }
}

countC_ND
countD_NC
```

i)  how many of parametrisation result in p(\|a,c) is monotone in C but p(y\|a,d) is not monotone in d

ii) p(y∣a, d) is monotone in D but p(y∣a, c) is not monotone in C

```{r}

set.seed(1234)
library(ggplot2)

arrows <- c("^", ">", "v", "<")
action_deltas <- list(c(1,0), # up
                      c(0,1), # right
                      c(-1,0), # down
                      c(0,-1)) # left

vis_environment <- function(iterations=0, epsilon = 0.5, alpha = 0.1, gamma = 0.95, beta = 0, method ="undefined"){

  df <- expand.grid(x=1:H,y=1:W)
  foo <- mapply(function(x,y) ifelse(reward_map[x,y] == 0,q_table[x,y,1],NA),df$x,df$y)
  df$val1 <- as.vector(round(foo, 2))
  foo <- mapply(function(x,y) ifelse(reward_map[x,y] == 0,q_table[x,y,2],NA),df$x,df$y)
  df$val2 <- as.vector(round(foo, 2))
  foo <- mapply(function(x,y) ifelse(reward_map[x,y] == 0,q_table[x,y,3],NA),df$x,df$y)
  df$val3 <- as.vector(round(foo, 2))
  foo <- mapply(function(x,y) ifelse(reward_map[x,y] == 0,q_table[x,y,4],NA),df$x,df$y)
  df$val4 <- as.vector(round(foo, 2))
  foo <- mapply(function(x,y) 
    ifelse(reward_map[x,y] == 0,arrows[GreedyPolicy(x,y)],reward_map[x,y]),df$x,df$y)
  df$val5 <- as.vector(foo)
  foo <- mapply(function(x,y) ifelse(reward_map[x,y] == 0,max(q_table[x,y,]),
                                     ifelse(reward_map[x,y]<0,NA,reward_map[x,y])),df$x,df$y)
  df$val6 <- as.vector(foo)
  
  print(ggplot(df,aes(x = y,y = x)) +
          scale_fill_gradient(low = "white", high = "green", na.value = "red", name = "") +
          geom_tile(aes(fill=val6)) +
          geom_text(aes(label = val1),size = 4,nudge_y = .35,na.rm = TRUE) +
          geom_text(aes(label = val2),size = 4,nudge_x = .35,na.rm = TRUE) +
          geom_text(aes(label = val3),size = 4,nudge_y = -.35,na.rm = TRUE) +
          geom_text(aes(label = val4),size = 4,nudge_x = -.35,na.rm = TRUE) +
          geom_text(aes(label = val5),size = 10) +
          geom_tile(fill = 'transparent', colour = 'black') + 
          ggtitle(paste("Q-table after ",iterations," iterations\n",
                        "(epsilon = ",epsilon,", alpha = ",alpha,", method = ",method,"gamma = ",gamma,", beta = ",beta,")")) +
          theme(plot.title = element_text(hjust = 0.5)) +
          scale_x_continuous(breaks = c(1:W),labels = c(1:W)) +
          scale_y_continuous(breaks = c(1:H),labels = c(1:H)))
}

GreedyPolicy <- function(x, y){
  # Get the Q-values for all actions at state (x, y)
  q_values <- q_table[x, y, ]
  
  # Find the max Q-value
  max_q <- max(q_values)
  
  # Identify all actions with maximum Q-value
  max_actions <- which(q_values == max_q)
  
  # Check and resolve ties
  if (length(max_actions) > 1) {
    action <- sample(max_actions, 1)
  } else {
    action <- max_actions
  }
  return(action)
}

EpsilonGreedyPolicy <- function(x, y, epsilon){
  # Generate a random numb
  rand_num <- runif(1)
  if (rand_num < epsilon){
   #select a random action
    action <- sample(1:4, 1)
  } else {
  # use the greedy policy
    action <- GreedyPolicy(x, y)
  }
  return(action)
}

transition_model <- function(x, y, action, beta){
  delta <- sample(-1:1, size = 1, prob = c(0.5*beta,1-beta,0.5*beta))
  final_action <- ((action + delta + 3) %% 4) + 1
  foo <- c(x,y) + unlist(action_deltas[final_action])
  foo <- pmax(c(1,1),pmin(foo,c(H,W)))
  return (foo)
}


SARSA <- function(start_state, epsilon = 0.5, alpha = 0.1, gamma = 0.95, beta = 0, test = 0){
  x <- start_state[1]
  y <- start_state[2]
  episode_reward <- 0
  episode_correction <- 0
  iter = 0
  repeat{
    action <- EpsilonGreedyPolicy(x, y, epsilon*(1-test))
    next_state <- transition_model(x, y, action, beta)
    x_new <- next_state[1]
    y_new <- next_state[2]
    R <- reward_map[x_new, y_new]
    Q_SA <- q_table[x, y, action]
    next_action = EpsilonGreedyPolicy(x_new, y_new, epsilon*(1-test))
    Q_SA_prime <- q_table[x_new, y_new, next_action]
    TD_correction <- ifelse(R==0,-1,R) + gamma * Q_SA_prime - Q_SA
    episode_correction <- episode_correction + TD_correction*(1-test)
    q_table[x, y, action] <<- Q_SA + alpha * TD_correction*(1-test)
    episode_reward <- R
    x <- x_new
    y <- y_new
    if (R != 0){
      return (c(episode_reward-iter, episode_correction))
    } else {
      iter = iter +1
    }
  }
}


q_learning <- function(start_state, epsilon = 0.5, alpha = 0.1, gamma = 0.95, beta = 0, test = 0){
  x <- start_state[1]
  y <- start_state[2]
  iter = 0
  episode_reward <- 0
  episode_correction <- 0
  repeat{
    action <- EpsilonGreedyPolicy(x, y, epsilon*(1-test))
    next_state <- transition_model(x, y, action, beta)
    x_new <- next_state[1]
    y_new <- next_state[2]
    R <- reward_map[x_new, y_new]
    Q_SA <- q_table[x, y, action]
    max_QSAprime <- max(q_table[x_new, y_new, ])
    TD_correction <- ifelse(R==0,-1,R) + gamma * max_QSAprime - Q_SA
    episode_correction <- episode_correction + TD_correction*(1-test)
    q_table[x, y, action] <<- Q_SA + alpha * TD_correction*(1-test)
    episode_reward <- R
    x <- x_new
    y <- y_new
     if (R != 0){
      return (c(episode_reward-iter, episode_correction))
    } else {
      iter = iter +1
    }
  }
}


```

```{r}

H <- 3
W <- 6

MovingAverage <- function(x, n){
  cx <- c(0,cumsum(x))
  rsum <- (cx[(n+1):length(cx)] - cx[1:(length(cx) - n)]) / n
  return (rsum)
}

reward_map <- matrix(0, nrow = H, ncol = W)
reward_map[1,2:5] <- -10
reward_map[1,6] <- 10

#init
q_table <- array(0,dim = c(H,W,4))
vis_environment()

   
rewardQtr <- NULL   
#----Q-learning-----#   
for(i in 1:5000) {
  foo <- q_learning(epsilon = 0.5, gamma = 1, start_state = c(1,1), test = 0)
  rewardQtr <- c(rewardQtr,foo[1])
}
vis_environment(i, epsilon = 0.5, gamma = 1, alpha = 0.1, beta = 0, method = "q-learning - training")

rewardQte <- NULL   
for(i in 1:5000) {
    foo <- q_learning(epsilon = 0.5, gamma = 1, start_state = c(1,1), test = 1)
    rewardQte <- c(rewardQte,foo[1])
}
vis_environment(i, epsilon = 0.5, gamma = 1, alpha = 0.1, beta = 0, method = "q-learning testing")


# reset Q-table for sarsa
q_table <- array(0,dim = c(H,W,4))



rewardStr <- NULL
#---SARSA-----#   
  for(i in 1:5000) {
    foo <- SARSA(epsilon = 0.5, gamma = 1, start_state = c(1,1), test = 0)
    rewardStr <- c(rewardStr,foo[1])
  }
vis_environment(i, epsilon = 0.5, gamma = 1, alpha = 0.1, beta = 0, method = "SARSA training")

rewardSte <- NULL
#---SARSA-----#   
  for(i in 1:5000) {
    foo <- SARSA(epsilon = 0.5, gamma = 1, start_state = c(1,1), test = 1)
    rewardSte <- c(rewardSte,foo[1])
  }
vis_environment(i, epsilon = 0.5, gamma = 1, alpha = 0.1, beta = 0, method = "SARSA testing")



plot(MovingAverage(rewardQtr,100),type = "l", main = "reward comparision", ylim = c(-14, 8))
lines(MovingAverage(rewardQte,100),type = "l", col="blue")
lines(MovingAverage(rewardStr,100),type = "l",col="red")
lines(MovingAverage(rewardSte,100),type = "l", col="green")
```

Which performs the best and why? From the graph it can be seen that SARSA performsa better during training than Q-learning but worse during testing. This is due to the fact that Q-learning takes the shortest route during training but falls of the cliff sometimes due to epsilon. Q-learning prefers the shortest path since it assumes in the update rule that it will act greedily but actually it will act epsilon greedily. During testing it performs better however due to that epsilon is set to 0 so it never falls off the cliff and takes the sortest path.

## 3. Gaussian processes (7p)

```{r}
# Squared Exponential Kernel Function
SquaredExpKernel <- function(x1,x2,sigmaF=1,ell=3){
  n1 <- length(x1)
  n2 <- length(x2)
  K <- matrix(NA,n1,n2)
  for (i in 1:n2){
    K[,i] <- sigmaF^2*exp(-0.5*( (x1-x2[i])/ell)^2 )
  }
  return(K)
}

# Posterior GP Function
posteriorGP <- function(X, y, XStar, sigmaNoise, k, ...) {
  n = length(X)
  K <- k(X, X, ...)  # Compute the covariance matrix
  kStar <- k(X, XStar, ...) # Compute covariance
  K_y <- K + sigmaNoise^2 * diag(length(X)) # Add noise variance to diagonal
  L <- t(chol(K_y))  
  alpha <- solve(t(L), solve(L, y))   # Solve for alpha
  fStar_mean <- t(kStar) %*% alpha   # Compute posterior mean
  v <- solve(L, kStar)   # Compute v = solve(L, kStar)
  V_fStar <- k(XStar, XStar, ...) - t(v) %*% v # pred variance (cov matrix)
log_marg_likelihood = -(1/2)*t(y)%*%alpha - sum(log(diag(L))) - (n/2)*log(2*pi)
  
  return(list(mean = fStar_mean, variance = V_fStar, log_likelihood = log_marg_likelihood))
}
```


```{r}
#  observations
X <- c(-1.0, -0.6, -0.2, 0.4, 0.8)
y <- c(0.768, -0.044, -0.940, 0.719, -0.664)

# Test inputs over the interval [-1, 1]
XStar <- seq(-1, 1, length.out = 201)

# Hyperparameters
sigmaF <- 1        # sigma_f
ell <- 0.3         # length-scale l
sigmaNoise <- 0  # sigma_n

XStar[101]
# Call posteriorGP
res <- posteriorGP(X, y, XStar, sigmaNoise, k = SquaredExpKernel, sigmaF = sigmaF, ell = ell)

plot(X, y)
lines(XStar, res$variance[101,], type = "l", lwd = 2)


```
Q: Why is the posterior covariance zero at the training points ?
A: the posterior cov is zero at the training points because sigmaNoise = 0. this means that the function has to go though them. 

Q: Why does the posterior covariance not decrease as a function of the distance
between x and x′ ?
A: Because it is constrain by the fact that it has to be zero at the training points

```{r}

foo <- SquaredExpKernel(XStar,0)
plot(XStar,foo)
```

