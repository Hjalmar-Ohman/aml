## 1. Graphical models

1)  

```{r}

library(bnlearn)
library(gRain)
dag = model2network("[C][D|C][A|C][Y|A:C]")

graphviz.plot(dag)
```

2)  

```{r}

countC_ND = 0
countD_NC = 0
for (i in 1:1000) {
  dag = model2network("[C][D|C][A|C][Y|A:C]")
  # C
  s = runif(1)
  cptC = c(s, 1-s)
  dim(cptC) = c(2)
  dimnames(cptC) = list(C = c("C1", "C0"))

  # D | C
  s1 = runif(1)
  s2 = runif(1)
  cptD = matrix(c(s1, 1-s1,  
                 s2, 1-s2),
               nrow = 2)
  dimnames(cptD) = list(D = c("D1", "D0"), C = c("C1", "C0"))

   # A | C
  s1 = runif(1)
  s2 = runif(1)
  cptA = matrix(c(s1, 1-s1,  
                 s2, 1-s2),
               nrow = 2)
  dimnames(cptA) = list(A = c("A1", "A0"), C = c("C1", "C0"))
  
     # Y | A,C
  s1 = runif(1)
  s2 = runif(1)
  s3 = runif(1)
  s4 = runif(1)
  cptY = matrix(c(s1,1-s1,
                  s2,1-s2,
                  s3,1-s3,
                  s4,1-s4),
                )
  dim(cptY) = c(2,2,2)
  dimnames(cptY) = list(Y = c("Y1", "Y0"), A = c("A1", "A0"), C = c("C1", "C0"))

  fit = custom.fit(dag, list(C = cptC,D = cptD,A = cptA, Y = cptY))
  model = compile(as.grain(fit))
  
  # for p(y|a,c)
  Y1_A1C1 <- querygrain(setEvidence(object = model, nodes = c("A", "C"), states = c("A1", "C1")),nodes = "Y")$Y[1]
  Y1_A1C0 <- querygrain(setEvidence(object = model, nodes = c("A", "C"), states = c("A1", "C0")),nodes = "Y")$Y[1]
  Y1_A0C1 <- querygrain(setEvidence(object = model, nodes = c("A", "C"), states = c("A0", "C1")),nodes = "Y")$Y[1]
  Y1_A0C0 <- querygrain(setEvidence(object = model, nodes = c("A", "C"), states = c("A0", "C0")),nodes = "Y")$Y[1]
  
  # for p(y|a,d)
  Y1_A1D1 <- querygrain(setEvidence(object = model, nodes = c("A", "D"), states = c("A1", "D1")),nodes = "Y")$Y[1]
  Y1_A1D0 <- querygrain(setEvidence(object = model, nodes = c("A", "D"), states = c("A1", "D0")),nodes = "Y")$Y[1]
  Y1_A0D1 <- querygrain(setEvidence(object = model, nodes = c("A", "D"), states = c("A0", "D1")),nodes = "Y")$Y[1]
  Y1_A0D0 <- querygrain(setEvidence(object = model, nodes = c("A", "D"), states = c("A0", "D0")),nodes = "Y")$Y[1]
 
  if(Y1_A1C1 >= Y1_A1C0 && Y1_A0C1 >= Y1_A0C0){
    if(Y1_A1C1 <= Y1_A1C0 && Y1_A0C1 <= Y1_A0C0){
      monotoneC = TRUE
    }
    } else {
    monotoneC = FALSE
  }
 
      monotoneD = FALSE
   if(Y1_A1D1 >= Y1_A1D0 && Y1_A0D1 >= Y1_A0D0){
    if(Y1_A1D1 <= Y1_A1D0 && Y1_A0D1 <= Y1_A0D0){
      monotoneD = TRUE
    }
    } else {
    monotoneD = FALSE
  }
 
 
  if(monotoneC && !monotoneD){
    countC_ND = countC_ND +1
  }else if(!monotoneC && monotoneD){
    countD_NC = countD_NC +1 
  }
}

countC_ND
countD_NC
```

i)  how many of parametrisation result in p(\|a,c) is monotone in C but p(y\|a,d) is not monotone in d

ii) p(y∣a, d) is monotone in D but p(y∣a, c) is not monotone in C

```{r}

set.seed(1234)
library(ggplot2)

arrows <- c("^", ">", "v", "<")
action_deltas <- list(c(1,0), # up
                      c(0,1), # right
                      c(-1,0), # down
                      c(0,-1)) # left

vis_environment <- function(iterations=0, epsilon = 0.5, alpha = 0.1, gamma = 0.95, beta = 0){

  df <- expand.grid(x=1:H,y=1:W)
  foo <- mapply(function(x,y) ifelse(reward_map[x,y] == 0,q_table[x,y,1],NA),df$x,df$y)
  df$val1 <- as.vector(round(foo, 2))
  foo <- mapply(function(x,y) ifelse(reward_map[x,y] == 0,q_table[x,y,2],NA),df$x,df$y)
  df$val2 <- as.vector(round(foo, 2))
  foo <- mapply(function(x,y) ifelse(reward_map[x,y] == 0,q_table[x,y,3],NA),df$x,df$y)
  df$val3 <- as.vector(round(foo, 2))
  foo <- mapply(function(x,y) ifelse(reward_map[x,y] == 0,q_table[x,y,4],NA),df$x,df$y)
  df$val4 <- as.vector(round(foo, 2))
  foo <- mapply(function(x,y) 
    ifelse(reward_map[x,y] == 0,arrows[GreedyPolicy(x,y)],reward_map[x,y]),df$x,df$y)
  df$val5 <- as.vector(foo)
  foo <- mapply(function(x,y) ifelse(reward_map[x,y] == 0,max(q_table[x,y,]),
                                     ifelse(reward_map[x,y]<0,NA,reward_map[x,y])),df$x,df$y)
  df$val6 <- as.vector(foo)
  
  print(ggplot(df,aes(x = y,y = x)) +
          scale_fill_gradient(low = "white", high = "green", na.value = "red", name = "") +
          geom_tile(aes(fill=val6)) +
          geom_text(aes(label = val1),size = 4,nudge_y = .35,na.rm = TRUE) +
          geom_text(aes(label = val2),size = 4,nudge_x = .35,na.rm = TRUE) +
          geom_text(aes(label = val3),size = 4,nudge_y = -.35,na.rm = TRUE) +
          geom_text(aes(label = val4),size = 4,nudge_x = -.35,na.rm = TRUE) +
          geom_text(aes(label = val5),size = 10) +
          geom_tile(fill = 'transparent', colour = 'black') + 
          ggtitle(paste("Q-table after ",iterations," iterations\n",
                        "(epsilon = ",epsilon,", alpha = ",alpha,"gamma = ",gamma,", beta = ",beta,")")) +
          theme(plot.title = element_text(hjust = 0.5)) +
          scale_x_continuous(breaks = c(1:W),labels = c(1:W)) +
          scale_y_continuous(breaks = c(1:H),labels = c(1:H)))
}

GreedyPolicy <- function(x, y){
  # Get the Q-values for all actions at state (x, y)
  q_values <- q_table[x, y, ]
  
  # Find the max Q-value
  max_q <- max(q_values)
  
  # Identify all actions with maximum Q-value
  max_actions <- which(q_values == max_q)
  
  # Check and resolve ties
  if (length(max_actions) > 1) {
    action <- sample(max_actions, 1)
  } else {
    action <- max_actions
  }
  return(action)
}

EpsilonGreedyPolicy <- function(x, y, epsilon){
  # Generate a random numb
  rand_num <- runif(1)
  if (rand_num < epsilon){
   #select a random action
    action <- sample(1:4, 1)
  } else {
  # use the greedy policy
    action <- GreedyPolicy(x, y)
  }
  return(action)
}

transition_model <- function(x, y, action, beta){
  delta <- sample(-1:1, size = 1, prob = c(0.5*beta,1-beta,0.5*beta))
  final_action <- ((action + delta + 3) %% 4) + 1
  foo <- c(x,y) + unlist(action_deltas[final_action])
  foo <- pmax(c(1,1),pmin(foo,c(H,W)))
  return (foo)
}

SARSA <- function(start_state, epsilon = 0.5, alpha = 0.1, gamma = 0.95, beta = 0){
  x <- start_state[1]
  y <- start_state[2]
  episode_reward <- 0
  episode_correction <- 0
  repeat{
    action <- EpsilonGreedyPolicy(x, y, epsilon)
    next_state <- transition_model(x, y, action, beta)
    x_new <- next_state[1]
    y_new <- next_state[2]
    R <- reward_map[x_new, y_new]
    Q_SA <- q_table[x, y, action]
      R = ifelse(R==0,-1, reward)
    TD_correction <- R + gamma * q_table[x_new, y_new, ] - Q_SA
    episode_correction <- episode_correction + TD_correction
    q_table[x, y, action] <<- Q_SA + alpha * TD_correction
    episode_reward <- episode_reward + R
    x <- x_new
    y <- y_new
    if (R != 0){
      return (c(episode_reward, episode_correction))
    }
  }
}


q_learning <- function(start_state, epsilon = 0.5, alpha = 0.1, gamma = 0.95, beta = 0){
  x <- start_state[1]
  y <- start_state[2]
  episode_reward <- 0
  episode_correction <- 0
  repeat{
    action <- EpsilonGreedyPolicy(x, y, epsilon)
    next_state <- transition_model(x, y, action, beta)
    x_new <- next_state[1]
    y_new <- next_state[2]
    R <- reward_map[x_new, y_new]
    Q_SA <- q_table[x, y, action]
    max_QSAprime <- max(q_table[x_new, y_new, ])
    TD_correction <- R + gamma * max_QSAprime - Q_SA
    episode_correction <- episode_correction + TD_correction
    q_table[x, y, action] <<- Q_SA + alpha * TD_correction
    episode_reward <- episode_reward + R
    x <- x_new
    y <- y_new
    if (R != 0){
      return (c(episode_reward, episode_correction))
    }
  }
}


```

```{r}

H <- 3
W <- 6
MovingAverage <- function(x, n){
  cx <- c(0,cumsum(x))
  rsum <- (cx[(n+1):length(cx)] - cx[1:(length(cx) - n)]) / n
  return (rsum)
}

reward_map <- matrix(0, nrow = H, ncol = W)
reward_map[1,2:5] <- -10
reward_map[1,6] <- 10

q_table <- array(0,dim = c(H,W,4))
vis_environment()
q_table <- array(0,dim = c(H,W,4))
   reward <- NULL
  for(i in 1:5000) {
    foo <- q_learning(epsilon = 0.5, gamma = 1, start_state = c(1,1))
    reward <- c(reward,foo[1])
  }

vis_environment(i, epsilon = 0.5, gamma = 1, alpha = 0.1, beta = 0)
plot(MovingAverage(reward,100),type = "l")
# report final q table and policy

```
