---
title: "Solution"
author: "Hjalmar Öhman"
date: "2024-10-21"
output: pdf_document
---

# 1

```{r}
rm(list=ls())
library(bnlearn)
library(gRain)
set.seed(123)
data("asia")
id=1:10


full_data=asia[id,]
true_dag = model2network("[A][S][T|A][L|S][B|S][D|B:E][E|T:L][X|E]")
full_fit = bn.fit(x = true_dag, data = full_data, method="bayes")
full_fit$D
full_grain = as.grain(full_fit)
full_compile = compile(full_grain)

miss_data=asia[-id,]
miss_data[, "B"] = NULL
miss_data[, "E"] = NULL


for (i in 1:nrow(miss_data)) {
  evidence = setEvidence(full_compile, colnames(miss_data), as.vector(as.matrix(miss_data[i, ])))
  query = querygrain(evidence, c("B"))$B
  miss_data[i,"B"] = sample(c("no","yes"),size=1,prob=query)
  
  query = querygrain(evidence, c("E"))$E
  miss_data[i,"E"] = sample(c("no","yes"),size=1,prob=query)
}

inference_bn<-bn.fit(true_dag,rbind(full_data, miss_data),method="bayes") # Imputed parameters
inference_bn$D

true_fit<-bn.fit(true_dag,asia,method="bayes") # "True" Asia parameters
true_fit$D
```

### 2 

```{r}
library(HMM)

States=c("1a","1b","2a","2b","2c","3a","3b","4","5a","5b")
Symbols=c("1","2","3","4","5")
transProbs=matrix(c(.5,.5,0,0,0,0,0,0,0,0,
                    0,.5,.5,0,0,0,0,0,0,0,
                    0,0,.5,.5,0,0,0,0,0,0,
                    0,0,0,.5,.5,0,0,0,0,0,
                    0,0,0,0,.5,.5,0,0,0,0,
                    0,0,0,0,0,.5,.5,0,0,0,
                    0,0,0,0,0,0,.5,.5,0,0,
                    0,0,0,0,0,0,0,.5,.5,0,
                    0,0,0,0,0,0,0,0,.5,.5,
                    .5,0,0,0,0,0,0,0,0,.5), nrow=length(States), ncol=length(States), byrow = TRUE)
emissionProbs=matrix(c(1/3,1/3,0,0,1/3,
                       1/3,1/3,0,0,1/3,
                       1/3,1/3,1/3,0,0,
                       1/3,1/3,1/3,0,0,
                       1/3,1/3,1/3,0,0,
                       0,1/3,1/3,1/3,0,
                       0,1/3,1/3,1/3,0,
                       0,0,1/3,1/3,1/3,
                       1/3,0,0,1/3,1/3,
                       1/3,0,0,1/3,1/3), nrow=length(States), ncol=length(Symbols), byrow = TRUE)
startProbs=c(.1,.1,.1,.1,.1,.1,.1,.1,.1,.1)
hmm=initHMM(States,Symbols,startProbs,transProbs,emissionProbs)
sim=simHMM(hmm,100)
sim
```

### 3 
```{r}
rm(list = ls())
# By Jose M. Peña and Joel Oskarsson.
# For teaching purposes.
# jose.m.pena@liu.se.
#####################################################################################################
# Q-learning
#####################################################################################################
# install.packages("ggplot2")
# install.packages("vctrs")
library(ggplot2)
set.seed(1234)
arrows <- c("ˆ", ">", "v", "<")
action_deltas <- list(c(1,0), # up
c(0,1), # right
c(-1,0), # down
c(0,-1)) # left
vis_environment <- function(iterations=0, epsilon = 0.5, alpha = 0.1, gamma = 0.95, beta = 0){
  # Visualize an environment with rewards.
  # Q-values for all actions are displayed on the edges of each tile.
  # The (greedy) policy for each state is also displayed.
  #
  # Args:
  # iterations, epsilon, alpha, gamma, beta (optional): for the figure title.
  # reward_map (global variable): a HxW array containing the reward given at each state.
  # q_table (global variable): a HxWx4 array containing Q-values for each state-action pair.
  # H, W (global variables): environment dimensions.
  df <- expand.grid(x=1:H,y=1:W)
  foo <- mapply(function(x,y) ifelse(reward_map[x,y] == 0,q_table[x,y,1],NA),df$x,df$y)
  df$val1 <- as.vector(round(foo, 2))
  foo <- mapply(function(x,y) ifelse(reward_map[x,y] == 0,q_table[x,y,2],NA),df$x,df$y)
  df$val2 <- as.vector(round(foo, 2))
  foo <- mapply(function(x,y) ifelse(reward_map[x,y] == 0,q_table[x,y,3],NA),df$x,df$y)
  df$val3 <- as.vector(round(foo, 2))
  foo <- mapply(function(x,y) ifelse(reward_map[x,y] == 0,q_table[x,y,4],NA),df$x,df$y)
  df$val4 <- as.vector(round(foo, 2))
  foo <- mapply(function(x,y)
  ifelse(reward_map[x,y] == 0,arrows[GreedyPolicy(x,y)],reward_map[x,y]),df$x,df$y)
  df$val5 <- as.vector(foo)
  foo <- mapply(function(x,y) ifelse(reward_map[x,y] == 0,max(q_table[x,y,]),
  ifelse(reward_map[x,y]<0,NA,reward_map[x,y])),df$x,df$y)
  df$val6 <- as.vector(foo)
  print(ggplot(df,aes(x = y,y = x)) +
          scale_fill_gradient(low = "white", high = "green", na.value = "red", name = "") +
          geom_tile(aes(fill=val6)) +
          geom_text(aes(label = val1),size = 4,nudge_y = .35,na.rm = TRUE) +
          geom_text(aes(label = val2),size = 4,nudge_x = .35,na.rm = TRUE) +
          geom_text(aes(label = val3),size = 4,nudge_y = -.35,na.rm = TRUE) +
          geom_text(aes(label = val4),size = 4,nudge_x = -.35,na.rm = TRUE) +
          geom_text(aes(label = val5),size = 10) +
          geom_tile(fill = 'transparent', colour = 'black') +
          ggtitle(paste("Q-table after ",iterations," iterations\n",
                        "(epsilon = ",epsilon,", alpha = ",alpha,"gamma = ",gamma,", beta = ",beta,")")) +
          theme(plot.title = element_text(hjust = 0.5)) +
          scale_x_continuous(breaks = c(1:W),labels = c(1:W)) +
          scale_y_continuous(breaks = c(1:H),labels = c(1:H)))
}

GreedyPolicy <- function(x, y){
  # Get a greedy action for state (x,y) from q_table.
  #
  # Args:
  # x, y: state coordinates.
  # q_table (global variable): a HxWx4 array containing Q-values for each state-action pair.
  #
  # Returns:
  # An action, i.e. integer in {1,2,3,4}.
  # Your code here.
  q_values = q_table[x, y, ]
  best_actions = which(q_values == max(q_values))
  if (length(best_actions) > 1) {
  action = sample(best_actions, 1)
  } else {
  action = best_actions
  }
  return (action)
}

EpsilonGreedyPolicy <- function(x, y, epsilon){
  # Get an epsilon-greedy action for state (x,y) from q_table.
  #
  # Args:
  # x, y: state coordinates.
  # epsilon: probability of acting randomly.
  #
  # Returns:
  # An action, i.e. integer in {1,2,3,4}.
  # Your code here.
  if (epsilon >= runif(1)){
    action = sample(1:4, 1)
  } else {
    action = GreedyPolicy(x, y)
  }
  return (action)
}

transition_model <- function(x, y, action, beta){
  # Computes the new state after given action is taken. The agent will follow the action
  # with probability (1-beta) and slip to the right or left with probability beta/2 each.
  #
  # Args:
  # x, y: state coordinates.
  # action: which action the agent takes (in {1,2,3,4}).
  # beta: probability of the agent slipping to the side when trying to move.
  # H, W (global variables): environment dimensions.
  #
  # Returns:
  # The new state after the action has been taken.
  delta <- sample(-1:1, size = 1, prob = c(0.5*beta,1-beta,0.5*beta))
  final_action <- ((action + delta + 3) %% 4) + 1
  foo <- c(x,y) + unlist(action_deltas[final_action])
  foo <- pmax(c(1,1),pmin(foo,c(H,W)))
  return (foo)
}

q_learning <- function(start_state, epsilon = 0.5, alpha = 0.1, gamma = 0.95, beta = 0){
  #Initialize
  current_state = start_state
  episode_correction = 0
  reward = 0
  repeat{
    # Current state
    x = current_state[1]
    y = current_state[2]
    # Action
    action = EpsilonGreedyPolicy(x, y, epsilon)
    # Update state
    next_state = transition_model(x, y, action, beta)
    next_x = next_state[1]
    next_y = next_state[2]
    # Reward
    reward = reward_map[next_x, next_y]
    # New max Q value
    max_q_next = max(q_table[next_x, next_y, ])
    # Correction
    correction = reward + gamma * max_q_next - q_table[x, y, action]
    # Update q_table based on correction
    q_table[x, y, action] <<- q_table[x, y, action] + alpha * correction
    # Accumulate corrections
    episode_correction = episode_correction + correction
    # Update state
    current_state = next_state
    # End the episode if a terminal state (non-zero reward) is reached
    if (reward != 0) {
      break
    }
  }
  return (c(reward, episode_correction))
}

set.seed(1234)
H <- 5
W <- 7
reward_map <- matrix(0, nrow = H, ncol = W)
reward_map[3,6] <- 10
reward_map[2:4,3] <- -1
q_table <- array(0,dim = c(H,W,4))

for (alpha_i in c(0.001, 0.01, 0.1)) {
  for(i in 1:500){
    foo <- q_learning(start_state = c(3,1), alpha = alpha_i, gamma=1)
  }
  vis_environment(i, alpha = alpha_i, gamma = 1)
}

```
Q: Report and analyze the results that you obtain.
A: Q-values are updated by larger amounts as alpha increases.