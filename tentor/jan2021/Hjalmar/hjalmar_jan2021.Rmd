### 1
```{r}

```

### 2

```{r}
rm(list = ls())
set.seed(1)
library(bnlearn)
library(gRain)
net = model2network("[Z0][X0|Z0][Z1|Z0][X1|Z1][Z2|Z1][X2|Z2][Z3|Z2]")
graphviz.plot(net)
states = 1:10
symbols = 1:10

cptZ0 = rep(.1, 10)
dim(cptZ0) = c(10)
dimnames(cptZ0) = list(c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10"))

# Define the transition probabilities
cptZ1 = matrix(0, nrow = length(states), ncol = length(states))
for (i in 1:(length(states) - 1)) {
  cptZ1[i, i] = 0.5 # Stay in the same sector
  cptZ1[i, i + 1] = 0.5 # Move to the next sector
}
# Last Sector transitions to Sector 1
cptZ1[length(states), length(states)] = 0.5
cptZ1[length(states), 1] = 0.5
dim(cptZ1) = c(10, 10)
dimnames(cptZ1) = list(
  "Z1" = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10"),
  "Z0"  = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10")
)

# Define the transition probabilities
cptZ2 = matrix(0, nrow = length(states), ncol = length(states))
for (i in 1:(length(states) - 1)) {
  cptZ2[i, i] = 0.5 # Stay in the same sector
  cptZ2[i, i + 1] = 0.5 # Move to the next sector
}
# Last Sector transitions to Sector 1
cptZ2[length(states), length(states)] = 0.5
cptZ2[length(states), 1] = 0.5
dim(cptZ2) = c(10, 10)
dimnames(cptZ2) = list(
  "Z2" = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10"),
  "Z1"  = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10")
)

# Define the transition probabilities
cptZ3 = matrix(0, nrow = length(states), ncol = length(states))
for (i in 1:(length(states) - 1)) {
  cptZ3[i, i] = 0.5 # Stay in the same sector
  cptZ3[i, i + 1] = 0.5 # Move to the next sector
}
# Last Sector transitions to Sector 1
cptZ3[length(states), length(states)] = 0.5
cptZ3[length(states), 1] = 0.5
dim(cptZ3) = c(10, 10)
dimnames(cptZ3) = list(
  "Z3" = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10"),
  "Z2"  = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10")
)


# Define the emission probabilities
cptX0 = matrix(0, nrow = length(states), ncol = length(symbols))
for (i in 1:length(states)) {
  # Get the sectors in the range [i-2, i+2]
  sectors = c((i - 2):(i + 2)) %% length(symbols) # (-1%%10 transforms -1 to 9)
  sectors[sectors == 0] = length(symbols) # (10%%10 transfroms 10 to 0, which is incorrect.)
  cptX0[i, sectors] = 1 / 5
  # Equal probability for the 5 neighboring sectors
}
dim(cptX0) = c(10, 10)
dimnames(cptX0) = list(
  "X0" = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10"),
  "Z0"  = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10")
)

# Define the emission probabilities
cptX1 = matrix(0, nrow = length(states), ncol = length(symbols))
for (i in 1:length(states)) {
  # Get the sectors in the range [i-2, i+2]
  sectors = c((i - 2):(i + 2)) %% length(symbols) # (-1%%10 transforms -1 to 9)
  sectors[sectors == 0] = length(symbols) # (10%%10 transfroms 10 to 0, which is incorrect.)
  cptX1[i, sectors] = 1 / 5
  # Equal probability for the 5 neighboring sectors
}
dim(cptX1) = c(10, 10)
dimnames(cptX1) = list(
  "X1" = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10"),
  "Z1"  = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10")
)

# Define the emission probabilities
cptX2 = matrix(0, nrow = length(states), ncol = length(symbols))
for (i in 1:length(states)) {
  # Get the sectors in the range [i-2, i+2]
  sectors = c((i - 2):(i + 2)) %% length(symbols) # (-1%%10 transforms -1 to 9)
  sectors[sectors == 0] = length(symbols) # (10%%10 transfroms 10 to 0, which is incorrect.)
  cptX2[i, sectors] = 1 / 5
  # Equal probability for the 5 neighboring sectors
}
dim(cptX2) = c(10, 10)
dimnames(cptX2) = list(
  "X2" = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10"),
  "Z2"  = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10")
)

net = model2network("[Z0][X0|Z0][Z1|Z0][X1|Z1][Z2|Z1][X2|Z2][Z3|Z2]")
fit = custom.fit(net, list(Z0 = cptZ0, X0 = cptX0, Z1 = cptZ1, X1 = cptX1, Z2 = cptZ2, X2 = cptX2, Z3 = cptZ3))
fit = compile(as.grain(fit))
which.max(querygrain(setEvidence(fit, c("X0", "X2"), c("1", "3")))$Z0)
which.max(querygrain(setEvidence(fit, c("X0", "X2"), c("1", "3")))$Z1)
which.max(querygrain(setEvidence(fit, c("X0", "X2"), c("1", "3")))$Z2)
which.max(querygrain(setEvidence(fit, c("X0", "X2"), c("1", "3")))$Z3)
```

## 3

```{r}

library(ggplot2)

arrows <- c("^", ">", "v", "<")
action_deltas <- list(c(1,0), # up
                      c(0,1), # right
                      c(-1,0), # down
                      c(0,-1)) # left

vis_environment <- function(iterations=0, epsilon = 0.5, alpha = 0.1, gamma = 0.95, beta = 0){
  
  # Visualize an environment with rewards. 
  # Q-values for all actions are displayed on the edges of each tile.
  # The (greedy) policy for each state is also displayed.
  # 
  # Args:
  #   iterations, epsilon, alpha, gamma, beta (optional): for the figure title.
  #   reward_map (global variable): a HxW array containing the reward given at each state.
  #   q_table (global variable): a HxWx4 array containing Q-values for each state-action pair.
  #   H, W (global variables): environment dimensions.
  
  df <- expand.grid(x=1:H,y=1:W)
  foo <- mapply(function(x,y) ifelse(reward_map[x,y] == 0,q_table[x,y,1],NA),df$x,df$y)
  df$val1 <- as.vector(round(foo, 2))
  foo <- mapply(function(x,y) ifelse(reward_map[x,y] == 0,q_table[x,y,2],NA),df$x,df$y)
  df$val2 <- as.vector(round(foo, 2))
  foo <- mapply(function(x,y) ifelse(reward_map[x,y] == 0,q_table[x,y,3],NA),df$x,df$y)
  df$val3 <- as.vector(round(foo, 2))
  foo <- mapply(function(x,y) ifelse(reward_map[x,y] == 0,q_table[x,y,4],NA),df$x,df$y)
  df$val4 <- as.vector(round(foo, 2))
  foo <- mapply(function(x,y) 
    ifelse(reward_map[x,y] == 0,arrows[GreedyPolicy(x,y)],reward_map[x,y]),df$x,df$y)
  df$val5 <- as.vector(foo)
  foo <- mapply(function(x,y) ifelse(reward_map[x,y] == 0,max(q_table[x,y,]),
                                     ifelse(reward_map[x,y]<0,NA,reward_map[x,y])),df$x,df$y)
  df$val6 <- as.vector(foo)
  
  print(ggplot(df,aes(x = y,y = x)) +
          scale_fill_gradient(low = "white", high = "green", na.value = "red", name = "") +
          geom_tile(aes(fill=val6)) +
          geom_text(aes(label = val1),size = 4,nudge_y = .35,na.rm = TRUE) +
          geom_text(aes(label = val2),size = 4,nudge_x = .35,na.rm = TRUE) +
          geom_text(aes(label = val3),size = 4,nudge_y = -.35,na.rm = TRUE) +
          geom_text(aes(label = val4),size = 4,nudge_x = -.35,na.rm = TRUE) +
          geom_text(aes(label = val5),size = 10) +
          geom_tile(fill = 'transparent', colour = 'black') + 
          ggtitle(paste("Q-table after ",iterations," iterations\n",
                        "(epsilon = ",epsilon,", alpha = ",alpha,"gamma = ",gamma,", beta = ",beta,")")) +
          theme(plot.title = element_text(hjust = 0.5)) +
          scale_x_continuous(breaks = c(1:W),labels = c(1:W)) +
          scale_y_continuous(breaks = c(1:H),labels = c(1:H)))
  
}

GreedyPolicy <- function(x, y){
  
  # Get a greedy action for state (x,y) from q_table.
  #
  # Args:
  #   x, y: state coordinates.
  #   q_table (global variable): a HxWx4 array containing Q-values for each state-action pair.
  # 
  # Returns:
  #   An action, i.e. integer in {1,2,3,4}.
  
  foo <- which(q_table[x,y,] == max(q_table[x,y,]))
  return (ifelse(length(foo)>1,sample(foo, size = 1),foo))
  
}

EpsilonGreedyPolicy <- function(x, y, epsilon){
  
  # Get an epsilon-greedy action for state (x,y) from q_table.
  #
  # Args:
  #   x, y: state coordinates.
  #   epsilon: probability of acting randomly.
  # 
  # Returns:
  #   An action, i.e. integer in {1,2,3,4}.
  
  foo <- sample(0:1,size = 1,prob = c(epsilon,1-epsilon))
  return (ifelse(foo == 1,GreedyPolicy(x,y),sample(1:4,size = 1)))
  
}

transition_model <- function(x, y, action, beta){
  
  # Computes the new state after given action is taken. The agent will follow the action 
  # with probability (1-beta) and slip to the right or left with probability beta/2 each.
  # 
  # Args:
  #   x, y: state coordinates.
  #   action: which action the agent takes (in {1,2,3,4}).
  #   beta: probability of the agent slipping to the side when trying to move.
  #   H, W (global variables): environment dimensions.
  # 
  # Returns:
  #   The new state after the action has been taken.
  
  delta <- sample(-1:1, size = 1, prob = c(0.5*beta,1-beta,0.5*beta))
  final_action <- ((action + delta + 3) %% 4) + 1
  foo <- c(x,y) + unlist(action_deltas[final_action])
  foo <- pmax(c(1,1),pmin(foo,c(H,W)))
  
  return (foo)
}

q_learning <- function(start_state, epsilon = 0.5, alpha = 0.1, gamma = 0.95, 
                       beta = 0, tr = 1){
  
  # Perform one episode of Q-learning. The agent should move around in the 
  # environment using the given transition model and update the Q-table.
  # The episode ends when the agent reaches a terminal state.
  # 
  # Args:
  #   start_state: array with two entries, describing the starting position of the agent.
  #   epsilon (optional): probability of acting greedily.
  #   alpha (optional): learning rate.
  #   gamma (optional): discount factor.
  #   beta (optional): slipping factor.
  #   reward_map (global variable): a HxW array containing the reward given at each state.
  #   q_table (global variable): a HxWx4 array containing Q-values for each state-action pair.
  # 
  # Returns:
  #   reward: reward received in the episode.
  #   correction: sum of the temporal difference correction terms over the episode.
  #   q_table (global variable): Recall that R passes arguments by value. So, q_table being
  #   a global variable can be modified with the superassigment operator <<-.
  
  cur_pos <- start_state
  episode_correction <- 0
  
  repeat{
    # Follow policy, execute action, get reward.
    action <- EpsilonGreedyPolicy(cur_pos[1], cur_pos[2], epsilon*tr)
    new_pos <- transition_model(cur_pos[1], cur_pos[2], action, beta)
    reward <- reward_map[new_pos[1], new_pos[2]]
    
    # Q-table update.
    old_q <- q_table[cur_pos[1], cur_pos[2], action]
    correction <- reward + gamma*max(q_table[new_pos[1], new_pos[2], ]) - old_q
    q_table[cur_pos[1], cur_pos[2], action] <<- old_q + alpha*correction*tr
    
    cur_pos <- new_pos
    episode_correction <- episode_correction + correction*tr
    
    if(reward!=0)
      # End episode.
      return (c(reward,episode_correction))
  }
  
}

#SARSARSA
SARSA2 <- function(start_state, epsilon = 0.5, alpha = 0.1, gamma = 0.95, 
                       beta = 0, tr = 1){

  cur_pos <- start_state
  cur_action <- EpsilonGreedyPolicy(cur_pos[1], cur_pos[2], epsilon*tr)
  
  new_pos <- transition_model(cur_pos[1], cur_pos[2], cur_action, beta)
  new_action <- EpsilonGreedyPolicy(new_pos[1], new_pos[2], epsilon*tr)
  reward <- reward_map[new_pos[1], new_pos[2]]

  episode_correction <- 0
  
  repeat{
    # Follow policy, execute action, get reward.
    new2_pos <- transition_model(new_pos[1], new_pos[2], new_action, beta)
    new2_action <- EpsilonGreedyPolicy(new2_pos[1], new2_pos[2], epsilon*tr)
    new_reward <- reward_map[new2_pos[1], new2_pos[2]]
    
    # Q-table update.
    old_q <- q_table[cur_pos[1], cur_pos[2], cur_action]
    correction <- ifelse(reward==0, -1, reward) + gamma*new_reward + gamma**2*q_table[new2_pos[1], new2_pos[2], new2_action] - old_q
    
    q_table[cur_pos[1], cur_pos[2], cur_action] <<- old_q + alpha*correction*tr
    
    if(reward!=0)
      # End episode.
      return (c(reward,episode_correction))
    
    cur_pos <- new_pos
    cur_action <- new_action
    new_pos <- new2_pos
    new_action <- new2_action
    reward <- new_reward
    
    episode_correction <- episode_correction + correction*tr
    
  }
  
}


```

```{r}
# Environment C (the effect of beta).
set.seed(1234)
H <- 3
W <- 6
reward_map <- matrix(0, nrow = H, ncol = W)
reward_map[1, 2:5] <- -10
reward_map[1, 6] <- 10
q_table <- array(0, dim = c(H, W, 4))


q_table <- array(0, dim = c(H, W, 4))
for (i in 1:5000) {
  foo <- SARSA2(epsilon = 0.5,
                gamma = 1,
                beta = 0,
                alpha = 0.1,
                start_state = c(1, 1))
}
vis_environment(iterations = 5000, 
                epsilon = 0.5,
                gamma = 1,
                beta = 0,
                alpha = 0.1)
```
