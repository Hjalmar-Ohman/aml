---
output:
  html_document: default
  pdf_document: default
---

### Probabilistic Graphical Models (5 p)

```{r}
# Load Required Libraries
library(bnlearn)
library(gRain)

# Define the Network Structure
net <- model2network("[U][C|U][A|C][B|C][D|A:B][Ch|U][Ah|Ch][Bh|Ch][Dh|Ah:Bh]")
graphviz.plot(net)

# Define CPTs with Consistent State Names ("0" and "1")

# 1. U: Court orders execution
cptU <- c(0.5, 0.5)
dim(cptU) <- c(2)
dimnames(cptU) <- list(U = c("0", "1"))

# 2. C: Captain orders fire | U
cptC <- matrix(c(0.9, 0.1,  # C=0 | U=0 and U=1
                 0.1, 0.9),
               nrow = 2)
dimnames(cptC) <- list(C = c("0", "1"), U = c("0", "1"))

# 3. A: Rifleman A shoots | C
cptA <- matrix(c(1, 0,      # A=0 | C=0 and C=1
                 0.2, 0.8),
               nrow = 2)
dimnames(cptA) <- list(A = c("0", "1"), C = c("0", "1"))

# 4. B: Rifleman B shoots | C
cptB <- matrix(c(1, 0,      # B=0 | C=0 and C=1
                 0.2, 0.8),
               nrow = 2)
dimnames(cptB) <- list(B = c("0", "1"), C = c("0", "1"))

# 5. D: Prisoner dies | A, B
cptD <- array(c(
  0.9, 0.1,  # D=0,1 | A=0, B=0
  0,   1,    # D=0,1 | A=0, B=1
  0,   1,    # D=0,1 | A=1, B=0
  0,   1     # D=0,1 | A=1, B=1
), dim = c(2, 2, 2),
dimnames = list(D = c("0", "1"), A = c("0", "1"), B = c("0", "1")))

# 6. Ch: Captain orders fire in hypothetical world | U
cptCh <- matrix(c(0.9, 0.1,  # Ch=0 | U=0 and U=1
                  0.1, 0.9),
                nrow = 2)
dimnames(cptCh) <- list(Ch = c("0", "1"), U = c("0", "1"))

# 7. Ah: Rifleman A shoots in hypothetical world | Ch
cptAh <- matrix(c(1, 0,      # Ah=0 | Ch=0 and Ch=1
                  0.2, 0.8),
                nrow = 2)
dimnames(cptAh) <- list(Ah = c("0", "1"), Ch = c("0", "1"))

# 8. Bh: Rifleman B shoots in hypothetical world | Ch
cptBh <- matrix(c(1, 0,      # Bh=0 | Ch=0 and Ch=1
                  0.2, 0.8),
                nrow = 2)
dimnames(cptBh) <- list(Bh = c("0", "1"), Ch = c("0", "1"))

# 9. Dh: Prisoner dies in hypothetical world | Ah, Bh
cptDh <- array(c(
  0.9, 0.1,  # Dh=0,1 | Ah=0, Bh=0
  0,   1,    # Dh=0,1 | Ah=0, Bh=1
  0,   1,    # Dh=0,1 | Ah=1, Bh=0
  0,   1     # Dh=0,1 | Ah=1, Bh=1
), dim = c(2, 2, 2),
dimnames = list(Dh = c("0", "1"), Ah = c("0", "1"), Bh = c("0", "1")))

# Combine CPTs into the Network
netfit <- custom.fit(net, list(
  U = cptU,
  C = cptC,
  A = cptA,
  B = cptB,
  D = cptD,
  Ch = cptCh,
  Ah = cptAh,
  Bh = cptBh,
  Dh = cptDh
))

# Compile the Network for Inference
netcom <- compile(as.grain(netfit))

# Set Evidence and Query
# Objective: Compute P(Dh = 1 | D = 1, Ah = 0)
# Here, D = 1 (Prisoner is dead in the actual world)
# Ah = 0 (Rifleman A did not shoot in the hypothetical world)
result <- querygrain(setEvidence(netcom, nodes = c("D", "Ah"), states = c("1", "0")), 
                     nodes = c("Dh"))

# Display the Result
print(result)
```

### Hidden markov models

```{r}
library(HMM)
rm(list = ls())

# Define the hidden states and observation symbols
states <- c("S1 C2", "S1 C1", "S2 C3", "S2 C2", "S2 C1", "S3 C2", "S3 C1", "S4 C1", "S5 C2", "S5 C1")
symbols <- c("S1", "S2", "S3", "S4", "S5")

# Initialize the initial state probabilities: the robot is equally likely to start in any sector
start_probs <- c(0.2, 0, 0.2, 0, 0, 0.2, 0, 0.2, 0.2, 0)

# Initialize the transition probability matrix with zeros
trans_probs <- matrix(0, nrow = 10, ncol = 10)

colnames(trans_probs) = states
rownames(trans_probs) = states

trans_probs["S1 C2", "S1 C1"] = 1
trans_probs["S1 C1", "S1 C1"] = 0.5
trans_probs["S1 C1", "S2 C3"] = 0.5
trans_probs["S2 C3", "S2 C2"] = 1
trans_probs["S2 C2", "S2 C1"] = 1
trans_probs["S2 C1", "S2 C1"] = 0.5
trans_probs["S2 C1", "S3 C2"] = 0.5
trans_probs["S3 C2", "S3 C1"] = 1
trans_probs["S3 C1", "S3 C1"] = 0.5
trans_probs["S3 C1", "S4 C1"] = 0.5
trans_probs["S4 C1", "S4 C1"] = 0.5
trans_probs["S4 C1", "S5 C2"] = 0.5
trans_probs["S5 C2", "S5 C1"] = 1
trans_probs["S5 C1", "S5 C1"] = 0.5
trans_probs["S5 C1", "S1 C2"] = 0.5

#states <- c("S1 C2", "S1 C1", "S2 C3", "S2 C2", "S2 C1", "S3 C2", "S3 C1", "S4 C1", "S5 C2", "S5 C1")
#symbols <- c("1", "2", "3", "4", "5")

# Initialize the emission probability matrix with zeros
emission_probs = matrix(c(
  1/3,1/3,0,0,1/3,
  1/3,1/3,0,0,1/3,
  1/3,1/3,1/3,0,0,
  1/3,1/3,1/3,0,0,
  1/3,1/3,1/3,0,0,
  0,1/3,1/3,1/3,0,
  0,1/3,1/3,1/3,0,
  0,0,1/3,1/3,1/3,
  1/3,0,0,1/3,1/3,
  1/3,0,0,1/3,1/3
)
, nrow = 10, ncol = 5, byrow = TRUE)

colnames(emission_probs) = symbols
rownames(emission_probs) = states

emission_probs
# Initialize the Hidden Markov Model
hmm_model <- initHMM(
  States = states,           # vector of states
  Symbols = symbols,         # vector of observation symbols
  startProbs = start_probs,  # Initial state probabilities
  transProbs = trans_probs,  # Transition probabilities matrix
  emissionProbs = emission_probs  # Emission probabilities matrix
)

set.seed(12345)
simulation <- simHMM(hmm_model, length = 100)
print(simulation)
```

### Reinforcement learning

```{r}
# S+ is all the states
# S all the non-terminal states

theta = 0.1
gamma = 0.95
V <- rep(0,10)
pi <- rep(0,10)
sectors = seq(1,10,1)
r = rep(0,10)
r[10] = 1 # reward for last state

repeat{
  delta = 0 
  for(s in sectors-1){
    v = V[s]
    stay = r[s] + V[s]*gamma
    move = r[s+1] + V[s+1]*gamma
    V[s] = max(stay, move)
    delta = max(delta, abs(v-V[s]))
  }
  if(delta < theta)break
   
}
  for(s in sectors-1){
    stay = r[s] + V[s]*gamma
    move = r[s+1] + V[s+1]*gamma
    pi[s] = which.max(c(stay, move))
  }


pi
V
```

### Gaussian processes

```{r}
# Preparing the data
data = read.csv("https://github.com/STIMALiU/AdvMLCourse/raw/master/GaussianProcess/Code/TempTullinge.csv", header=TRUE, sep=";")

time = seq(1,2190, 5)
day = seq(1,365, 5)
days = rep(day, 6)
data_sampled = data[time,]
temps = data_sampled$temp
```

```{r}
#LM

posteriorGP <- function(X, y, XStar, sigmaNoise, k, ...) {
  n = length(X)
  K <- k(X, X, ...)  # Compute the covariance matrix
  kStar <- k(X, XStar, ...) # Compute covariance
  
  # Step 2 in algo
  #--------------------
  K_y <- K + sigmaNoise^2 * diag(length(X)) # Add noise variance to diagonal
  L <- t(chol(K_y))   # Compute Cholesky decomposition, to get lower triangular L we take t()
  alpha <- solve(t(L), solve(L, y))   # Solve for alpha
  #---------------------
  
  # Step 4 in algo
  #--------------------
  fStar_mean <- t(kStar) %*% alpha   # Compute posterior mean
  v <- solve(L, kStar)   # Compute v = solve(L, kStar)
  #-------------------
  
  # Step 6 in algo
  #-------------------
  V_fStar <- k(XStar, XStar, ...) - t(v) %*% v # pred variance (cov matrix)
  #-------------------
  
log_marg_likelihood = -(1/2)*t(y)%*%alpha - sum(log(diag(L))) - (n/2)*log(2*pi)
  
  return(list(mean = fStar_mean, variance = V_fStar, log_likelihood = log_marg_likelihood))
}
```

```{r}



quad_model = lm(temps~days + I(days^2), data = data_sampled)


SEKernel <- function(x1,x2,params = c(0,0)){
  n1 <- length(x1)
  n2 <- length(x2)
  K <- matrix(NA,n1,n2)
  for (i in 1:n2){
    K[,i] <- (params[1]^2)*exp(-0.5*( (x1-x2[i])/params[2])^2 )
  }
  return(K)
}

SEKernel(c(2, 185, 365), c(2, 185, 365), c(20,90))

XStar <- seq(-1, 1, length.out = 100)

posteriorGP(X=time, y=temp, XStar, var(quad_model$residuals), k = SEKernel)
```
