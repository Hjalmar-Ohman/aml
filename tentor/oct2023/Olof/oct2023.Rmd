---
title: "oct2023"
author: "Olof Swedberg"
date: "2024-10-18"
output: html_document
---

# 1 Probabilistic Graphical Models

-   U: Court orders the execution in the actual and hypothetical worlds.
-   C: Captain orders fire in the actual world.
-   A: Rifleman A shoots in the actual world.
-   B: Rifleman B shoots in the actual world.
-   D: Prisoner dies in the actual world.
-   Ch: Captain orders fire in the hypothetical world.
-   Ah: Rifleman A shoots in the hypothetical world.
-   Bh: Rifleman B shoots in the hypothetical world.
-   Dh: Prisoner dies in the hypothetical world.

```{r}
library(bnlearn)
library(gRain)
#Naive assumption: independence between all features
arc.set = matrix(c("U", "C",
                   "C", "A",
                   "C", "B",
                   "A", "D",
                   "B", "D",
                   "U", "Ch",
                   "Ch", "Ah",
                   "Ch", "Bh",
                   "Ah", "Dh",
                   "Bh", "Dh"),
                 ncol = 2, byrow = TRUE,
                 dimnames = list(NULL, c("from", "to")))

net = empty.graph(c("U", "C", "A", "B", "D", "Ch", "Ah", "Bh", "Dh")) 
arcs(net) = arc.set

graphviz.plot(net)

# U: Prob that Court orders the execution in the actual  and hypothetical worlds
cptU <- c(.5,.5)
dim(cptU) <- c(2)
dimnames(cptU) <- list(c("0", "1"))
print(cptU)

############ REAL WORLD #############
# C: Prob that Captain orders fire in the actual world given U
cptC <- matrix(c(.9,.1,.1,.9), nrow=2, ncol=2)
dim(cptC) <- c(2,2)
dimnames(cptC) <- list("C" = c("0", "1"), "U" =  c("0", "1"))
print(cptC)

# A: Prob Rifleman A shoots in the actual world given C
cptA = matrix(c(1,0,0.2,0.8), ncol=2, nrow = 2)
dim(cptA) = c(2,2)
dimnames(cptA) = list("A" = c("0","1"), "C" = c("0","1"))
print(cptA)

# B: Prob Rifleman B shoots in the actual world given C
cptB = matrix(c(1,0,0.2,0.8), ncol=2, nrow = 2)
dim(cptB) = c(2,2)
dimnames(cptB) = list("B" = c("0","1"), "C" = c("0","1"))
print(cptB)

# D: Prob Prisoner dies in the actual world given A (1 = dies) and B
# Question here: how do we know the probability p(D=1|A=0, B=0) = 0.1?
cptD = matrix(c(0.9,0.1,0,1,0,1,0,1), nrow = 2, ncol = 4)
dim(cptD) = c(2,2,2)
dimnames(cptD) = list("D" = c("0", "1"), "A" = c("0","1"), "B" = c("0",1))
print(cptD)

############ HYPOTHETICAL WORLD #############
# Ch: Prob that Captain orders fire in the actual world given U
cptCh <- matrix(c(.9,.1,.1,.9), nrow=2, ncol=2)
dim(cptCh) <- c(2,2)
dimnames(cptCh) <- list("Ch" = c("0", "1"), "U" =  c("0", "1"))
print(cptCh)

# Ah: Prob Rifleman Ah shoots in the actual world given Ch
cptAh = matrix(c(1,0,0.2,0.8), ncol=2, nrow = 2)
dim(cptAh) = c(2,2)
dimnames(cptAh) = list("Ah" = c("0","1"), "Ch" = c("0","1"))
print(cptAh)

# Bh: Prob Rifleman Bh shoots in the actual world given Ch
cptBh = matrix(c(1,0,0.2,0.8), ncol=2, nrow = 2)
dim(cptBh) = c(2,2)
dimnames(cptBh) = list("Bh" = c("0","1"), "Ch" = c("0","1"))
print(cptBh)

# Dh: Prob Prisoner dies in the actual world given Ah (1 = dies) and Bh
# Question here: how do we know the probability p(Dh=1|Ah=0, Bh=0) = 0.1?
cptDh = matrix(c(0.9,0.1,0,1,0,1,0,1), nrow = 2, ncol = 4)
dim(cptDh) = c(2,2,2)
dimnames(cptDh) = list("Dh" = c("0", "1"), "Ah" = c("0","1"), "Bh" = c("0",1))
print(cptDh)


# Fit the network with the Conditional Probability Tables
netfit <- custom.fit(net,list(U=cptU, C=cptC, A=cptA, B=cptB, D=cptD, Ch=cptCh, Ah=cptAh, Bh=cptBh, Dh=cptDh))
# Compile to grain to be able to set evidence 
netcom <- compile(as.grain(netfit))

# Calclate the probability of the Prisoner being dead given hypothetical Gurad A did not shoot
# given that he is dead in the real world
evidence = querygrain(setEvidence(netcom,nodes=c("D","Ah"),states=c("1","0")))
evidence$Dh
```

# 2 Hidden Markov Models

```{r}
set.seed(12345) 
library(HMM)

# Sector X: Steps -- implement this with a counter
# Sector 1: 2
# Sector 2: 3
# Sector 3: 2
# Sector 4: 1
# Sector 5: 2

states=c("Z=1.C=2","Z=1.C=1",
         "Z=2.C=3","Z=2.C=2","Z=2.C=1",
         "Z=3.C=2","Z=3.C=1",
         "Z=4.C=1",
         "Z=5.C=2","Z=5.C=1")

emissionSymbols = 1:5
transitionProb = matrix(0,nrow = length(states), ncol = length(states)) 

transitionProb = matrix(c(0, 1, 0, 0, 0, 0, 0, 0, 0, 0, # Z=1.C=2
                          0,.5,.5, 0, 0, 0, 0, 0, 0, 0, # Z=1.C=1 Can stay or move forward 
                          0, 0, 0, 1, 0, 0, 0, 0, 0, 0, # Z=2.C=3
                          0, 0, 0, 0, 1, 0, 0, 0, 0, 0, # Z=2.C=2
                          0, 0, 0, 0,.5,.5, 0, 0, 0, 0, # Z=2.C=1 Can stay or move forward 
                          0, 0, 0, 0, 0, 0, 1, 0, 0, 0, # Z=3.C=2
                          0, 0, 0, 0, 0, 0,.5,.5, 0, 0, # Z=3.C=1 Can stay or move forward
                          0, 0, 0, 0, 0, 0, 0,.5,.5, 0, # Z=4.C=1 Can stay or move forward 
                          0, 0, 0, 0, 0, 0, 0, 0, 0, 1, # Z=5.C=2 
                          .5, 0, 0, 0, 0, 0, 0, 0, 0,.5), # Z=5.C=1 Can stay or move forward
                        nrow = length(states), ncol = length(states), byrow = TRUE)
b = 1/3
emissionProb = matrix(c(b, b, 0, 0, b, # Z=1.C=2
                        b, b, 0, 0, b, # Z=1.C=1 Can stay or move forward 
                        0, b, b, b, 0, # Z=2.C=3
                        0, b, b, b, 0, # Z=2.C=2
                        0, b, b, b, 0, # Z=2.C=1 Can stay or move forward 
                        0, 0, b, b, b, # Z=3.C=2
                        0, 0, b, b, b, # Z=3.C=1 Can stay or move forward
                        b, 0, 0, b, b, # Z=4.C=1 Can stay or move forward 
                        b, 0, 0, b, b, # Z=5.C=2 
                        b, b, 0, 0, b), # Z=5.C=1 Can stay or move forward
                        nrow = length(states), ncol = length(emissionSymbols), byrow = TRUE)

# Assuming we start at with full countdown  
startProb = c(.2, 0,.2, 0, 0,.2, 0, .2,.2,0)

hmm = initHMM(States = states, Symbols = emissionSymbols, startProbs = startProb, transProbs = transitionProb,
              emissionProbs = emissionProb)
nIter = 100
simulation = simHMM(hmm, nIter) 
print(simulation)

```

### Prove this expression:

$p(z^{T-1}|x^{0:T}) = \Sigma_{z^T} \frac{p(z^T|z^{T-1})p(z^{T-1}|x^{0:T})p(z^T|x^{0:T})}{\Sigma_{z^{T-1}} p(z^T|z^{T-1})p(z^{T-1}|x^{0:T-1})}$

#### Step 1: Marginalization over $z^T$

$p(z^{T-1}|x^{0:T})=\sum_{z^T} p(z^{T-1},z^T|x^{0:T})$

This step comes from the law of total probability, where you marginalize over the hidden state $z^T$ at time $T$. Essentially, you are summing over all possible values of $z^T$ to get the marginal distribution of $z^{T-1}$ conditioned on the observations.

#### Step 2: Conditional Probability Decomposition

$=\sum_{z^T} p(z^{T-1},z^T|x^{0:T}) =\sum_{z^T} p(z^{T-1}|z^T,x^{0:T}) p(z^T|x^{0:T})$

This is just applying the product rule of conditional probability. You are expressing the joint probability $\sum_{z^T} p(z^{T-1},z^T|x^{0:T})$ as the product of the conditional probability $\sum_{z^T}p(z^{T-1}|z^T,x^{0:T})$ and the marginal probability $p(z^T|x^{0:T})$.

#### Step 3: Exploit conditional independence

Next, simplify $\sum_{z^T} p(z^{T-1}|z^T,x^{0:T})$ Due to the Markov property (which assumes that the current state only depends on the previous state and is conditionally independent of earlier states given the current state), we can simplify it to the expression: $p(z^{T-1}|z^T,x^{0:T-1})$. The whole expression:

$=\sum_{z^T} p(z^{T-1}|z^T,x^{0:T}) p(z^T|x^{0:T}) = \sum_{z^T} p(z^{T-1}|z^T,x^{0:T-1}) p(z^T|x^{0:T})$

#### Step 4: Apply Bayes rule

Apply Bayes rule to the expression $p(z^{T-1}|z^T,x^{0:T-1})$ to decompose the term into known conditional probabilites.

$= \sum_{z^T} p(z^{T-1}|z^T,x^{0:T-1}) p(z^T|x^{0:T}) =\sum_{z^T} \frac{p(z^T|z^{T-1},x^{0:T-1}) p(z^{T-1}|x^{0:T-1}) p(z^T|x^{0:T})}{p(z^T|x^{0:T-1})}$

#### Step 5: Simplification Using Conditional Independence

Simplify $p(z^T|z^{T-1},x^{0:T-1})$ to $p(z^T|z^{T-1})$ Since the Markov property tells us that only $z^T$ only depends on $z^{T-1}$, not the full observation history:

$=\sum_{z^T} \frac{p(z^T|z^{T-1},x^{0:T-1}) p(z^{T-1}|x^{0:T-1}) p(z^T|x^{0:T})}{p(z^T|x^{0:T-1})} =\sum_{z^T} \frac{p(z^T|z^{T-1}) p(z^{T-1}|x^{0:T-1}) p(z^T|x^{0:T})}{p(z^T|x^{0:T-1})}$

#### Step 6: Denominator Expansion

In this step, expand $p(z^T|x^{0:T-1})$ using the law of probability

$=\sum_{z^T} \frac{p(z^T|z^{T-1}) p(z^{T-1}|x^{0:T-1}) p(z^T|x^{0:T})}{p(z^T|x^{0:T-1})}=\sum_{z^T} \frac{p(z^T|z^{T-1}) p(z^{T-1}|x^{0:T-1}) p(z^T|x^{0:T})}{\sum_{z^{T-1}} p(z^{T-1},z^T|x^{0:T-1})}$

#### Step 7: Expand even more

$=\sum_{z^T} \frac{p(z^T|z^{T-1}) p(z^{T-1}|x^{0:T-1}) p(z^T|x^{0:T})}{\sum_{z^{T-1}} p(z^{T-1},z^T|x^{0:T-1})}=\sum_{z^T} \frac{p(z^T|z^{T-1}) p(z^{T-1}|x^{0:T-1}) p(z^T|x^{0:T})}{\sum_{z^{T-1}} p(z^T|z^{T-1},x^{0:T-1}) p(z^{T-1}|x^{0:T-1})}$

#### Step 8: Use the Markov Property

$=\sum_{z^T} \frac{p(z^T|z^{T-1}) p(z^{T-1}|x^{0:T-1}) p(z^T|x^{0:T})}{\sum_{z^{T-1}} p(z^T|z^{T-1},x^{0:T-1}) p(z^{T-1}|x^{0:T-1})}=\sum_{z^T} \frac{p(z^T|z^{T-1}) p(z^{T-1}|x^{0:T-1}) p(z^T|x^{0:T})}{\sum_{z^{T-1}} p(z^T|z^{T-1}) p(z^{T-1}|x^{0:T-1})}$

#### Step 9: Conclusion

$p(z^{T-1}|x^{0:T}) =\sum_{z^T} \frac{p(z^T|z^{T-1}) p(z^{T-1}|x^{0:T-1}) p(z^T|x^{0:T})}{\sum_{z^{T-1}} p(z^T|z^{T-1}) p(z^{T-1}|x^{0:T-1})}$

# 3 Reinforcement Learning
```{r}

R = array(0,dim=10)
R[10] = 1
V = array(0, dim = 10) # state values
pi = array(0, dim = 10) # policies
theta = 0.1
gamma = 0.95

repeat{
  delta = 0
  for (s in 1:9) {
    v = V[s]
    V[s] = max(gamma*V[s], R[s+1] + gamma*V[s+1]) # Why add gamma in the first term like Jose?
    delta = max(delta, abs(v-V[s]))
  }
if(delta<theta) break
}

for(s in 1:9) {
  pi[s] <- which.max(c(gamma*V[s],R[s+1]+gamma*V[s+1])) 
  # Why add gamma in the first term like Jose?
}

V
pi

```

